import os
import logging
from typing import Dict, List, Optional, Any, Tuple
import json
import asyncio
from datetime import datetime
import httpx
import re
import aiohttp
import urllib.parse
import requests  # ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ requestså¯¼å…¥
from parser import SlideContent
import urllib3
import warnings

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# æˆ–è€…
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
logger = logging.getLogger(__name__)


class SimpleAgent:
    """å¢å¼ºå®¹é”™æ€§çš„çŸ¥è¯†æ‰©å±•æ™ºèƒ½ä½“"""

    def __init__(self):
        self.api_key = "sk-xeajczsypmkihgqahpcsidmhyvgddyrnxyzpediquhhavvwa"
        self.base_url = "https://api.siliconflow.cn/v1"
        self.model = "deepseek-ai/DeepSeek-V3.2-Exp"

        # æ·»åŠ å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        self.semaphore = asyncio.Semaphore(2)  # é™åˆ¶åŒæ—¶2ä¸ªè¯·æ±‚

        # åˆ›å»ºHTTPå®¢æˆ·ç«¯
        self.client = httpx.AsyncClient(
            timeout=180.0,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
        )

        # æ ¡éªŒå±‚é…ç½®
        self.validation_config = {
            "max_retries": 3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
            "consistency_check": True,  # ä¸€è‡´æ€§æ ¡éªŒ
            "fact_check": True,  # äº‹å®æ ¡éªŒ
            "format_check": True,  # æ ¼å¼æ ¡éªŒ
            "relevance_check": True,  # ç›¸å…³æ€§æ ¡éªŒ
        }

        # å¹»è§‰æ£€æµ‹å…³é”®è¯
        self.hallucination_keywords = [
            "æˆ‘æ— æ³•ç¡®å®š", "æˆ‘ä¸ç¡®å®š", "å¯èƒ½", "æˆ–è®¸", "å¤§æ¦‚", "æ®è¯´",
            "æ®æˆ‘æ‰€çŸ¥", "ä¸€èˆ¬æ¥è¯´", "é€šå¸¸æƒ…å†µä¸‹", "å¯èƒ½ä¸æ­£ç¡®",
            "ç¼ºä¹å…·ä½“ä¿¡æ¯", "ä¿¡æ¯ä¸å®Œæ•´", "éœ€è¦è¿›ä¸€æ­¥æ ¸å®"
        ]

        # å­¦æœ¯æœ¯è¯­è¯å…¸ï¼ˆå¯æ‰©å±•ï¼‰
        self.academic_terms = {
            # æœºå™¨å­¦ä¹ /ç»Ÿè®¡å­¦
            "è´å¶æ–¯", "æœ´ç´ è´å¶æ–¯", "è´å¶æ–¯å®šç†", "è´å¶æ–¯åˆ†ç±»å™¨",
            "åéªŒæ¦‚ç‡", "å…ˆéªŒæ¦‚ç‡", "ä¼¼ç„¶å‡½æ•°", "æå¤§ä¼¼ç„¶ä¼°è®¡", "æœ€å¤§ä¼¼ç„¶ä¼°è®¡",
            "è”åˆæ¦‚ç‡", "æ¡ä»¶æ¦‚ç‡", "æ¦‚ç‡åˆ†å¸ƒ", "æœŸæœ›é£é™©", "é£é™©æœ€å°åŒ–",
            "åˆ†ç±»å™¨", "æœºå™¨å­¦ä¹ ", "ç›‘ç£å­¦ä¹ ", "æ— ç›‘ç£å­¦ä¹ ",
            "å‚æ•°ä¼°è®¡", "ç‰¹å¾", "å®ä¾‹", "è®­ç»ƒé›†", "æµ‹è¯•é›†",
            "å‡†ç¡®ç‡", "å¬å›ç‡", "F1åˆ†æ•°", "æ··æ·†çŸ©é˜µ",

            # æ•°å­¦
            "å®šç†", "å¼•ç†", "æ¨è®º", "è¯æ˜", "æ¨å¯¼", "å…¬å¼", "æ–¹ç¨‹",
            "çŸ©é˜µ", "å‘é‡", "æ ‡é‡", "æ¢¯åº¦", "å¯¼æ•°", "ç§¯åˆ†",

            # è®¡ç®—æœºç§‘å­¦
            "ç®—æ³•", "æ•°æ®ç»“æ„", "å¤æ‚åº¦", "æ—¶é—´å¤æ‚åº¦", "ç©ºé—´å¤æ‚åº¦",
            "é€’å½’", "è¿­ä»£", "ä¼˜åŒ–", "æ”¶æ•›", "ç¦»æ•£", "è¿ç»­",

            # é€šç”¨å­¦æœ¯
            "å®šä¹‰", "æ¦‚å¿µ", "åŸç†", "æ–¹æ³•", "æŠ€æœ¯", "æ¨¡å‹",
            "æ¡†æ¶", "ä½“ç³»", "ç»“æ„", "æœºåˆ¶", "è¿‡ç¨‹", "ç³»ç»Ÿ"
        }

        # ä¸­è‹±æ–‡æœ¯è¯­æ˜ å°„ï¼ˆç”¨äºWikipediaæœç´¢ï¼‰
        self.term_mapping = {
            # æœºå™¨å­¦ä¹ /ç»Ÿè®¡å­¦
            "æœ´ç´ è´å¶æ–¯": "Naive Bayes",
            "è´å¶æ–¯": "Bayes",
            "è´å¶æ–¯å®šç†": "Bayes' theorem",
            "è´å¶æ–¯åˆ†ç±»å™¨": "Bayesian classifier",
            "åéªŒæ¦‚ç‡": "Posterior probability",
            "å…ˆéªŒæ¦‚ç‡": "Prior probability",
            "ä¼¼ç„¶å‡½æ•°": "Likelihood function",
            "æå¤§ä¼¼ç„¶ä¼°è®¡": "Maximum likelihood estimation",
            "æœ€å¤§ä¼¼ç„¶ä¼°è®¡": "Maximum likelihood estimation",
            "è”åˆæ¦‚ç‡": "Joint probability",
            "æ¡ä»¶æ¦‚ç‡": "Conditional probability",
            "æ¦‚ç‡åˆ†å¸ƒ": "Probability distribution",
            "æœŸæœ›é£é™©": "Expected risk",
            "é£é™©æœ€å°åŒ–": "Risk minimization",
            "åˆ†ç±»å™¨": "Classifier",
            "æœºå™¨å­¦ä¹ ": "Machine learning",
            "ç›‘ç£å­¦ä¹ ": "Supervised learning",
            "æ— ç›‘ç£å­¦ä¹ ": "Unsupervised learning",
            "å‚æ•°ä¼°è®¡": "Parameter estimation",
            "ç‰¹å¾": "Feature",
            "å®ä¾‹": "Instance",
            "è®­ç»ƒé›†": "Training set",
            "æµ‹è¯•é›†": "Test set",
            "å‡†ç¡®ç‡": "Accuracy",
            "å¬å›ç‡": "Recall",
            "F1åˆ†æ•°": "F1 score",
            "æ··æ·†çŸ©é˜µ": "Confusion matrix",

            # æ•°å­¦
            "å®šç†": "Theorem",
            "å¼•ç†": "Lemma",
            "æ¨è®º": "Corollary",
            "è¯æ˜": "Proof",
            "æ¨å¯¼": "Derivation",
            "å…¬å¼": "Formula",
            "æ–¹ç¨‹": "Equation",
            "çŸ©é˜µ": "Matrix",
            "å‘é‡": "Vector",
            "æ ‡é‡": "Scalar",
            "æ¢¯åº¦": "Gradient",
            "å¯¼æ•°": "Derivative",
            "ç§¯åˆ†": "Integral",

            # è®¡ç®—æœºç§‘å­¦
            "ç®—æ³•": "Algorithm",
            "æ•°æ®ç»“æ„": "Data structure",
            "å¤æ‚åº¦": "Complexity",
            "æ—¶é—´å¤æ‚åº¦": "Time complexity",
            "ç©ºé—´å¤æ‚åº¦": "Space complexity",
            "é€’å½’": "Recursion",
            "è¿­ä»£": "Iteration",
            "ä¼˜åŒ–": "Optimization",
            "æ”¶æ•›": "Convergence",
            "ç¦»æ•£": "Discrete",
            "è¿ç»­": "Continuous",

            # é€šç”¨å­¦æœ¯
            "å®šä¹‰": "Definition",
            "æ¦‚å¿µ": "Concept",
            "åŸç†": "Principle",
            "æ–¹æ³•": "Method",
            "æŠ€æœ¯": "Technique",
            "æ¨¡å‹": "Model",
            "æ¡†æ¶": "Framework",
            "ä½“ç³»": "System",
            "ç»“æ„": "Structure",
            "æœºåˆ¶": "Mechanism",
            "è¿‡ç¨‹": "Process"
        }

        logger.info("âœ… æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆï¼ˆå¸¦æ ¡éªŒå±‚ï¼‰")

    async def call_llm_with_validation(self, messages: List[Dict[str, str]],
                                       task_type: str = "general",
                                       expected_format: Optional[str] = None) -> Tuple[str, Dict[str, Any]]:
        """å¸¦æ ¡éªŒå±‚çš„LLMè°ƒç”¨"""
        # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®ä¸åŒçš„max_tokens
        token_limits = {
            "code_example": 3000,  # ä»£ç ç¤ºä¾‹éœ€è¦æ›´å¤štoken
            "explanation": 1500,
            "quiz": 1000,
            "references": 800,
            "extended_reading": 3000,  # ã€å…³é”®ä¿®å¤ã€‘çŸ¥è¯†æ·±åº¦æ¢ç´¢éœ€è¦æ›´å¤štoken
            "general": 1000
        }
        max_tokens = token_limits.get(task_type, 1000)

        validation_result = {
            "passed": False,
            "errors": [],
            "warnings": [],
            "retries": 0,
            "consistency_score": 0.0,
            "validation_details": {}
        }

        for retry in range(self.validation_config["max_retries"]):
            try:
                logger.info(f"è°ƒç”¨LLM APIï¼ˆç¬¬{retry + 1}æ¬¡å°è¯•ï¼‰ï¼Œä»»åŠ¡ç±»å‹: {task_type}")

                response = await self.client.post(
                    f"{self.base_url}/chat/completions",
                    json={
                        "model": self.model,
                        "messages": messages,
                        "temperature": 0.7 if retry == 0 else 0.3,  # é‡è¯•æ—¶é™ä½æ¸©åº¦
                        "max_tokens": max_tokens  # ä½¿ç”¨åŠ¨æ€tokené™åˆ¶
                    },
                    timeout=1200.0
                )

                if response.status_code == 200:
                    result = response.json()
                    content = result["choices"][0]["message"]["content"]

                    # æ‰§è¡Œæ ¡éªŒ
                    validation_passed, validation_details = await self._validate_response(
                        content, messages, task_type, expected_format
                    )

                    validation_result["retries"] = retry
                    validation_result["validation_details"] = validation_details

                    if validation_passed:
                        validation_result["passed"] = True
                        validation_result["consistency_score"] = validation_details.get("consistency_score", 0.0)
                        logger.info(f"âœ… LLMè°ƒç”¨æˆåŠŸï¼ˆé€šè¿‡æ ¡éªŒï¼‰ï¼Œè¿”å›é•¿åº¦: {len(content)}")
                        return content, validation_result
                    else:
                        validation_result["errors"].extend(validation_details.get("errors", []))
                        validation_result["warnings"].extend(validation_details.get("warnings", []))

                        # å¦‚æœæ ¡éªŒå¤±è´¥ï¼Œå‡†å¤‡é‡è¯•
                        if retry < self.validation_config["max_retries"] - 1:
                            logger.warning(f"âš ï¸ ç¬¬{retry + 1}æ¬¡è°ƒç”¨æœªé€šè¿‡æ ¡éªŒï¼Œå‡†å¤‡é‡è¯•")

                            # æ·»åŠ æ ¡éªŒåé¦ˆåˆ°æ¶ˆæ¯ä¸­
                            feedback_msg = self._create_validation_feedback(validation_details)
                            messages.append({
                                "role": "user",
                                "content": f"ä¹‹å‰çš„å›ç­”å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼Œè¯·é‡æ–°å›ç­”ï¼š\n{feedback_msg}"
                            })
                        else:
                            logger.error(f"âŒ LLMè°ƒç”¨æœªé€šè¿‡æ ¡éªŒï¼ˆæœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰")
                            return content, validation_result
                else:
                    error_msg = f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}"
                    validation_result["errors"].append(error_msg)
                    logger.error(error_msg)

                    if retry < self.validation_config["max_retries"] - 1:
                        logger.info(f"ç­‰å¾…é‡è¯•...")
                        await asyncio.sleep(2 ** retry)  # æŒ‡æ•°é€€é¿

            except httpx.TimeoutException:
                error_msg = f"LLMè°ƒç”¨è¶…æ—¶ï¼ˆç¬¬{retry + 1}æ¬¡ï¼‰"
                validation_result["errors"].append(error_msg)
                logger.error(error_msg)

                if retry < self.validation_config["max_retries"] - 1:
                    await asyncio.sleep(2 ** retry)
            except Exception as e:
                error_msg = f"è°ƒç”¨LLMå¤±è´¥: {str(e)}"
                validation_result["errors"].append(error_msg)
                logger.error(error_msg, exc_info=True)

                if retry < self.validation_config["max_retries"] - 1:
                    await asyncio.sleep(2 ** retry)

        return "", validation_result

    async def _validate_response(self, content: str, messages: List[Dict[str, str]],
                                 task_type: str, expected_format: Optional[str]) -> Tuple[bool, Dict[str, Any]]:
        """æ ¡éªŒLLMå“åº”"""
        validation_details = {
            "passed_checks": [],
            "failed_checks": [],
            "errors": [],
            "warnings": [],
            "consistency_score": 0.0,
            "hallucination_detected": False
        }

        # 1. åŸºç¡€æ ¡éªŒ
        if not content or len(content.strip()) < 10:
            validation_details["failed_checks"].append("content_too_short")
            validation_details["errors"].append("å“åº”å†…å®¹å¤ªçŸ­æˆ–ä¸ºç©º")
            return False, validation_details

        validation_details["passed_checks"].append("content_length")

        # 2. æ ¼å¼æ ¡éªŒ
        if self.validation_config["format_check"] and expected_format:
            format_valid, format_errors = self._check_format(content, expected_format)
            if not format_valid:
                validation_details["failed_checks"].append("format_invalid")
                validation_details["errors"].extend(format_errors)
                return False, validation_details
            validation_details["passed_checks"].append("format_valid")

        # 3. å¹»è§‰æ£€æµ‹
        hallucination_detected = self._detect_hallucination(content)
        if hallucination_detected:
            validation_details["warnings"].append("æ£€æµ‹åˆ°å¯èƒ½çš„ä¸ç¡®å®šè¡¨è¿°")
            validation_details["hallucination_detected"] = True

        # 4. ä¸€è‡´æ€§æ ¡éªŒï¼ˆé’ˆå¯¹å¤šè½®å¯¹è¯ï¼‰
        if self.validation_config["consistency_check"] and len(messages) > 1:
            consistency_score = self._check_consistency(content, messages[-2]["content"])
            validation_details["consistency_score"] = consistency_score
            if consistency_score < 0.7:
                validation_details["warnings"].append(f"ä¸€è‡´æ€§å¾—åˆ†è¾ƒä½: {consistency_score:.2f}")

        # 5. äº‹å®æ€§æ ¡éªŒï¼ˆåŸºæœ¬æ£€æŸ¥ï¼‰
        if self.validation_config["fact_check"]:
            fact_issues = self._check_facts(content)
            if fact_issues:
                validation_details["warnings"].extend(fact_issues)

        # 6. ç›¸å…³æ€§æ ¡éªŒ
        if self.validation_config["relevance_check"]:
            user_query = self._extract_user_query(messages)
            if user_query:
                relevance_score = self._check_relevance(content, user_query)
                if relevance_score < 0.6:
                    validation_details["warnings"].append(f"ç›¸å…³æ€§å¾—åˆ†è¾ƒä½: {relevance_score:.2f}")

        return len(validation_details["failed_checks"]) == 0, validation_details

    def _check_format(self, content: str, expected_format: str) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥å“åº”æ ¼å¼"""
        errors = []

        if expected_format == "quiz_question":
            # æ£€æŸ¥æµ‹éªŒé—®é¢˜æ ¼å¼
            if "é—®é¢˜ï¼š" not in content:
                errors.append("ç¼ºå°‘é—®é¢˜æ ‡é¢˜")
            if "ç­”æ¡ˆï¼š" not in content:
                errors.append("ç¼ºå°‘ç­”æ¡ˆéƒ¨åˆ†")

            # æ£€æŸ¥é€‰æ‹©é¢˜é€‰é¡¹æ ¼å¼
            if "A." in content and not all(x in content for x in ["B.", "C.", "D."]):
                errors.append("é€‰æ‹©é¢˜é€‰é¡¹ä¸å®Œæ•´")

        elif expected_format == "explanation":
            # æ£€æŸ¥è§£é‡Šæ€§å†…å®¹çš„æ ¼å¼
            sections = ["æ ¸å¿ƒæ¦‚å¿µ", "åŸºæœ¬åŸç†", "åº”ç”¨åœºæ™¯", "çŸ¥è¯†å…³è”"]
            section_count = sum(1 for section in sections if section in content)
            if section_count < 2:
                errors.append(f"è§£é‡Šæ€§å†…å®¹ç»“æ„ä¸å®Œæ•´ï¼Œä»…åŒ…å«{section_count}ä¸ªä¸»è¦éƒ¨åˆ†")

        elif expected_format == "code_example":
            # æ£€æŸ¥ä»£ç ç¤ºä¾‹æ ¼å¼
            if "```" not in content:
                errors.append("ä»£ç ç¤ºä¾‹ç¼ºå°‘ä»£ç å—æ ‡è®°")
            if "def " not in content and "class " not in content and "print(" not in content:
                errors.append("ä»£ç ç¤ºä¾‹å¯èƒ½ä¸å®Œæ•´")

        elif expected_format == "extended_reading":
            # æ£€æŸ¥å»¶ä¼¸é˜…è¯»ææ–™çš„æ ¼å¼
            required_keywords = ["çŸ¥è¯†æ‰©å±•", "åº”ç”¨", "å­¦ä¹ ", "èƒŒæ™¯", "æ¡ˆä¾‹", "å»ºè®®"]
            found_keywords = sum(1 for keyword in required_keywords if keyword in content)
            if found_keywords < 2:
                errors.append(f"å»¶ä¼¸é˜…è¯»ææ–™å†…å®¹ä¸å¤Ÿä¸°å¯Œ")

        return len(errors) == 0, errors

    def _detect_hallucination(self, content: str) -> bool:
        """æ£€æµ‹å¹»è§‰å†…å®¹"""
        content_lower = content.lower()
        for keyword in self.hallucination_keywords:
            if keyword in content_lower:
                return True

        # æ£€æŸ¥è¿‡åº¦è‡ªä¿¡çš„è¡¨è¿°
        overconfident_patterns = [
            r"ç»å¯¹[æ˜¯|æ­£ç¡®]", r"è‚¯å®š[æ˜¯|æ­£ç¡®]", r"ç™¾åˆ†ä¹‹ç™¾", r"æ¯«æ— ç–‘é—®",
            r"å®Œå…¨[æ­£ç¡®|å‡†ç¡®]", r"ç»[ä¸|å¯¹]"
        ]

        for pattern in overconfident_patterns:
            if re.search(pattern, content):
                return True

        return False

    def _check_consistency(self, current_response: str, previous_content: str) -> float:
        """æ£€æŸ¥å“åº”ä¸€è‡´æ€§"""
        # ç®€åŒ–çš„åŸºäºå…³é”®è¯çš„ä¸€è‡´æ€§æ£€æŸ¥
        current_words = set(re.findall(r'\b\w{3,}\b', current_response.lower()))
        previous_words = set(re.findall(r'\b\w{3,}\b', previous_content.lower()))

        if not current_words or not previous_words:
            return 0.0

        intersection = current_words.intersection(previous_words)
        similarity = len(intersection) / max(len(current_words), len(previous_words))

        return similarity

    def _check_facts(self, content: str) -> List[str]:
        """åŸºæœ¬äº‹å®æ€§æ£€æŸ¥"""
        warnings = []

        # æ£€æŸ¥æ˜æ˜¾çš„é”™è¯¯äº‹å®ï¼ˆç¤ºä¾‹ï¼‰
        common_errors = {
            "Python 2": "Python 2å·²äº2020å¹´åœæ­¢æ”¯æŒ",
            "Java 7": "Java 7å·²åœæ­¢å…¬å…±æ›´æ–°",
            "Windows XP": "Windows XPå·²åœæ­¢æ”¯æŒ",
        }

        for error_term, correction in common_errors.items():
            if error_term in content and correction not in content:
                warnings.append(f"å¯èƒ½åŒ…å«è¿‡æ—¶ä¿¡æ¯: {error_term}")

        # æ£€æŸ¥çŸ›ç›¾çš„è¡¨è¿°
        contradictions = [
            ("åŒæ—¶", "ä½†æ˜¯"), ("è™½ç„¶", "ä½†æ˜¯"), ("ä¸€æ–¹é¢", "å¦ä¸€æ–¹é¢")
        ]

        for conj1, conj2 in contradictions:
            if conj1 in content and conj2 not in content:
                warnings.append(f"å¯èƒ½ç¼ºå°‘è½¬æŠ˜è¡¨è¿°: '{conj1}'å‡ºç°ä½†æœªæ‰¾åˆ°'{conj2}'")

        return warnings

    def _check_relevance(self, content: str, query: str) -> float:
        """æ£€æŸ¥å†…å®¹ç›¸å…³æ€§"""
        content_words = set(re.findall(r'\b\w{3,}\b', content.lower()))
        query_words = set(re.findall(r'\b\w{3,}\b', query.lower()))

        if not content_words or not query_words:
            return 0.0

        intersection = content_words.intersection(query_words)
        relevance = len(intersection) / len(query_words)

        return relevance

    def _extract_user_query(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """ä»æ¶ˆæ¯å†å²ä¸­æå–ç”¨æˆ·æŸ¥è¯¢"""
        for msg in reversed(messages):
            if msg["role"] == "user":
                return msg["content"]
        return None

    def _create_validation_feedback(self, validation_details: Dict[str, Any]) -> str:
        """åˆ›å»ºæ ¡éªŒåé¦ˆä¿¡æ¯"""
        feedback = []

        if validation_details.get("errors"):
            feedback.append("å‘ç°ä»¥ä¸‹é”™è¯¯ï¼š")
            feedback.extend([f"- {error}" for error in validation_details["errors"][:3]])

        if validation_details.get("warnings"):
            feedback.append("è¯·æ³¨æ„ä»¥ä¸‹é—®é¢˜ï¼š")
            feedback.extend([f"- {warning}" for warning in validation_details["warnings"][:3]])

        if validation_details.get("failed_checks"):
            feedback.append("æ ¼å¼è¦æ±‚ï¼š")
            for check in validation_details["failed_checks"]:
                if check == "format_invalid":
                    feedback.append("- è¯·ç¡®ä¿å“åº”æ ¼å¼æ­£ç¡®")
                elif check == "content_too_short":
                    feedback.append("- è¯·æä¾›æ›´è¯¦ç»†çš„å†…å®¹")

        return "\n".join(feedback)

    async def expand_slide(self, slide: SlideContent) -> Dict[str, Any]:
        """æ‰©å±•å•ä¸ªå¹»ç¯ç‰‡å†…å®¹ï¼ˆå¸¦æ ¡éªŒï¼‰"""
        try:
            # æ£€æŸ¥å¹»ç¯ç‰‡æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
            if not slide.title.strip() and not slide.content and not slide.bullet_points:
                logger.info(f"è·³è¿‡ç©ºå¹»ç¯ç‰‡: {slide.slide_number}")
                return {
                    "slide_number": slide.slide_number,
                    "title": slide.title,
                    "skipped": True,
                    "reason": "å†…å®¹ä¸ºç©º",
                    "explanations": [],
                    "examples": [],
                    "extended_readings": [],  # âœ… ç¡®ä¿å§‹ç»ˆåŒ…å«å»¶ä¼¸é˜…è¯»å­—æ®µ
                    "references": [],
                    "quiz_questions": [],
                    "expanded_at": datetime.now().isoformat()
                }

            logger.info(f"æ‰©å±•å¹»ç¯ç‰‡: {slide.slide_number} - {slide.title[:30]}")

            # å¹¶è¡Œæ‰§è¡Œæ‰©å±•ä»»åŠ¡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            tasks = [
                self._generate_explanation_with_validation(slide),  # è¯¦ç»†è§£é‡Š
                self._generate_examples_with_validation(slide),  # ä»£ç ç¤ºä¾‹
                self._generate_extended_reading_with_validation(slide),  # âœ… ç‹¬ç«‹çš„å»¶ä¼¸é˜…è¯»ææ–™
                self._generate_references_with_validation(slide),  # å‚è€ƒèµ„æ–™
                self._generate_quiz_with_validation(slide)  # æµ‹éªŒé—®é¢˜
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ğŸ”§ å®‰å…¨è·å–ç»“æœ
            def safe_get_result(index, task_name):
                try:
                    result = results[index]
                    if isinstance(result, Exception):
                        logger.error(f"{task_name}ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")
                        return [], {"passed": False, "errors": [str(result)], "warnings": ["ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸"]}
                    elif result is None:
                        logger.warning(f"{task_name}ä»»åŠ¡è¿”å›None")
                        return [], {"passed": False, "errors": ["è¿”å›ç»“æœä¸ºNone"]}
                    elif isinstance(result, tuple) and len(result) >= 2:
                        return result[0], result[1]
                    else:
                        logger.warning(f"{task_name}ä»»åŠ¡è¿”å›æ ¼å¼å¼‚å¸¸: {type(result)}")
                        return [], {"passed": False, "errors": ["è¿”å›æ ¼å¼å¼‚å¸¸"]}
                except Exception as e:
                    logger.error(f"è·å–{task_name}ç»“æœå¤±è´¥: {e}")
                    return [], {"passed": False, "errors": [str(e)]}

            explanation_result = safe_get_result(0, "è¯¦ç»†è§£é‡Š")
            examples_result = safe_get_result(1, "ä»£ç ç¤ºä¾‹")
            extended_reading_result = safe_get_result(2, "å»¶ä¼¸é˜…è¯»")  # âœ… é‡ç‚¹ç¡®ä¿è¿™ä¸ªéƒ¨åˆ†æ­£ç¡®
            references_result = safe_get_result(3, "å‚è€ƒèµ„æ–™")
            quiz_result = safe_get_result(4, "æµ‹éªŒé—®é¢˜")

            # ğŸ”§ å¤„ç†å»¶ä¼¸é˜…è¯»ææ–™ï¼Œç¡®ä¿ä¸€å®šæœ‰å†…å®¹
            extended_readings_content = extended_reading_result[0] if isinstance(extended_reading_result, tuple) and \
                                                                      extended_reading_result[0] is not None else []
            extended_reading_validation = extended_reading_result[1] if isinstance(extended_reading_result,
                                                                                   tuple) and len(
                extended_reading_result) > 1 else {"passed": False}

            # å¦‚æœå»¶ä¼¸é˜…è¯»ä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤å†…å®¹
            if not extended_readings_content or len(extended_readings_content) == 0:
                logger.warning(f"å¹»ç¯ç‰‡ {slide.slide_number} å»¶ä¼¸é˜…è¯»å†…å®¹ä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤å†…å®¹")

                # åˆ›å»ºé»˜è®¤å»¶ä¼¸é˜…è¯»ææ–™ - å‘½åä¸ºã€çŸ¥è¯†æ·±åº¦æ¢ç´¢ã€‘
                default_reading = {
                    "title": f"ã€Š{slide.title}ã€‹çŸ¥è¯†æ·±åº¦æ¢ç´¢",  # âœ… ä½¿ç”¨æ–°åç§°
                    "content": self._create_default_extended_reading(slide),
                    "sections": [
                        {"title": "çŸ¥è¯†æ·±åº¦æ‰©å±•", "content": f"æ·±å…¥æ¢è®¨{slide.title}ç›¸å…³çš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸç†ã€‚"},
                        {"title": "å†å²èƒŒæ™¯ä¸å‘å±•", "content": f"äº†è§£{slide.title}çš„å‘å±•å†å²å’Œé‡è¦é‡Œç¨‹ç¢‘ã€‚"},
                        {"title": "å®é™…åº”ç”¨æ¡ˆä¾‹", "content": f"åˆ—ä¸¾{slide.title}åœ¨å®é™…ä¸­çš„åº”ç”¨åœºæ™¯å’Œæ¡ˆä¾‹ã€‚"},
                        {"title": "å‰æ²¿è¿›å±•ä¸è¶‹åŠ¿", "content": f"ä»‹ç»è¯¥é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•å’Œå‘å±•è¶‹åŠ¿ã€‚"},
                        {"title": "æ·±å…¥å­¦ä¹ å»ºè®®", "content": f"æä¾›è¿›ä¸€æ­¥å­¦ä¹ {slide.title}çš„è·¯å¾„å’Œå»ºè®®ã€‚"}
                    ],
                    "type": "extended_reading",
                    "source": "ç³»ç»Ÿé»˜è®¤ç”Ÿæˆ",
                    "total_length": 800,
                    "section_count": 5,
                    "is_fallback": True,
                    "display_name": "çŸ¥è¯†æ·±åº¦æ¢ç´¢"  # âœ… åœ¨æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„åç§°
                }
                extended_readings_content = [default_reading]
                extended_reading_validation = {"passed": True, "warnings": ["ä½¿ç”¨é»˜è®¤å»¶ä¼¸é˜…è¯»å†…å®¹"]}
            else:
                # ä¸ºç°æœ‰çš„å»¶ä¼¸é˜…è¯»ææ–™æ·»åŠ æ˜¾ç¤ºåç§°
                for reading in extended_readings_content:
                    reading["display_name"] = reading.get("display_name", "çŸ¥è¯†æ·±åº¦æ¢ç´¢")
                    # ç¡®ä¿æ¯ä¸ªå»¶ä¼¸é˜…è¯»éƒ½æœ‰æ ‡é¢˜
                    if "title" not in reading or not reading["title"]:
                        reading["title"] = f"ã€Š{slide.title}ã€‹çŸ¥è¯†æ·±åº¦æ¢ç´¢"

            # å¤„ç†ç»“æœå¹¶è®°å½•æ ¡éªŒä¿¡æ¯
            expanded_content = {
                "slide_number": slide.slide_number,
                "title": slide.title,
                "explanations": explanation_result[0] if isinstance(explanation_result, tuple) else [],
                "examples": examples_result[0] if isinstance(examples_result, tuple) else [],
                "extended_readings": extended_readings_content,  # âœ… ç¡®ä¿è¿™ä¸ªå­—æ®µä¸€å®šæœ‰å†…å®¹
                "references": references_result[0] if isinstance(references_result, tuple) else [],
                "quiz_questions": quiz_result[0] if isinstance(quiz_result, tuple) else [],
                "expanded_at": datetime.now().isoformat(),
                "validation_summary": {
                    "explanation": explanation_result[1] if isinstance(explanation_result, tuple) and len(
                        explanation_result) > 1 else {"passed": False},
                    "examples": examples_result[1] if isinstance(examples_result, tuple) and len(
                        examples_result) > 1 else {"passed": False},
                    "extended_reading": extended_reading_validation,  # âœ… ç‹¬ç«‹çš„æ ¡éªŒç»“æœ
                    "references": references_result[1] if isinstance(references_result, tuple) and len(
                        references_result) > 1 else {"passed": False},
                    "quiz": quiz_result[1] if isinstance(quiz_result, tuple) and len(quiz_result) > 1 else {
                        "passed": False},
                }
            }

            # è®¡ç®—æ•´ä½“æ ¡éªŒé€šè¿‡ç‡
            validation_scores = []
            for key in ["explanation", "examples", "extended_reading", "references", "quiz"]:
                if key in expanded_content["validation_summary"]:
                    val = expanded_content["validation_summary"][key]
                    if isinstance(val, dict) and val.get("passed"):
                        validation_scores.append(1.0)
                    else:
                        validation_scores.append(0.0)

            if validation_scores:
                expanded_content["validation_score"] = sum(validation_scores) / len(validation_scores)

            # ğŸ”§ è°ƒè¯•ä¿¡æ¯
            logger.info(f"âœ… å¹»ç¯ç‰‡æ‰©å±•å®Œæˆ: {slide.slide_number}")
            logger.info(f"  è¯¦ç»†è§£é‡Š: {len(expanded_content['explanations'])} ä¸ª")
            logger.info(f"  ä»£ç ç¤ºä¾‹: {len(expanded_content['examples'])} ä¸ª")
            logger.info(f"  çŸ¥è¯†æ·±åº¦æ¢ç´¢: {len(expanded_content['extended_readings'])} ä¸ª")  # âœ… ä½¿ç”¨æ–°åç§°è®°å½•
            logger.info(f"  å‚è€ƒèµ„æ–™: {len(expanded_content['references'])} ä¸ª")
            logger.info(f"  æµ‹éªŒé—®é¢˜: {len(expanded_content['quiz_questions'])} ä¸ª")

            if expanded_content.get("extended_readings"):
                first_reading = expanded_content["extended_readings"][0]
                logger.info(f"  ğŸ“– çŸ¥è¯†æ·±åº¦æ¢ç´¢æ ‡é¢˜: {first_reading.get('title', 'æ— æ ‡é¢˜')}")
                logger.info(f"  ğŸ“„ çŸ¥è¯†æ·±åº¦æ¢ç´¢é•¿åº¦: {len(first_reading.get('content', ''))} å­—ç¬¦")

            return expanded_content

        except Exception as e:
            logger.error(f"æ‰©å±•å¹»ç¯ç‰‡å¤±è´¥: {e}", exc_info=True)
            # å³ä½¿å¤±è´¥ä¹Ÿè¿”å›åŒ…å«å»¶ä¼¸é˜…è¯»å­—æ®µçš„ç»“æ„
            return {
                "slide_number": slide.slide_number,
                "title": slide.title,
                "error": str(e),
                "explanations": [],
                "examples": [],
                "extended_readings": [{  # âœ… ç¡®ä¿å»¶ä¼¸é˜…è¯»å­—æ®µå§‹ç»ˆå­˜åœ¨
                    "title": f"ã€Š{slide.title}ã€‹çŸ¥è¯†æ·±åº¦æ¢ç´¢",  # âœ… ä½¿ç”¨æ–°åç§°
                    "content": self._create_error_extended_reading(slide, e),
                    "sections": [{"title": "é”™è¯¯ä¿¡æ¯", "content": f"ç”Ÿæˆå¤±è´¥: {str(e)}"}],
                    "type": "error_fallback",
                    "source": "é”™è¯¯æ¢å¤",
                    "is_fallback": True,
                    "display_name": "çŸ¥è¯†æ·±åº¦æ¢ç´¢"  # âœ… åœ¨æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„åç§°
                }],
                "references": [],
                "quiz_questions": [],
                "validation_score": 0.0,
                "validation_summary": {}
            }

    def _create_default_extended_reading(self, slide: SlideContent) -> str:
        """åˆ›å»ºé»˜è®¤å»¶ä¼¸é˜…è¯»ææ–™å†…å®¹"""
        keywords = self._extract_keywords_from_slide(slide)[:3]
        keyword_str = "ã€".join(keywords) if keywords else "ç›¸å…³ä¸»é¢˜"

        return f"""# ã€Š{slide.title}ã€‹çŸ¥è¯†æ·±åº¦æ¢ç´¢

## ğŸ“š çŸ¥è¯†æ·±åº¦æ‰©å±•
æœ¬éƒ¨åˆ†æ·±å…¥æ¢è®¨{slide.title}ç›¸å…³çš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸç†ã€‚{keyword_str}æ˜¯è¿™ä¸€é¢†åŸŸçš„é‡è¦çŸ¥è¯†ç‚¹ï¼Œå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶ã€‚

## ğŸ•°ï¸ å†å²èƒŒæ™¯ä¸å‘å±•
äº†è§£{slide.title}çš„å‘å±•å†å²æœ‰åŠ©äºæ·±å…¥ç†è§£å…¶å½“å‰åº”ç”¨ã€‚è¯¥çŸ¥è¯†é¢†åŸŸç»å†äº†å¤šä¸ªå‘å±•é˜¶æ®µï¼Œä»æœ€åˆçš„ç†è®ºæå‡ºåˆ°ç°åœ¨çš„å¹¿æ³›åº”ç”¨ã€‚

## ğŸ’¼ å®é™…åº”ç”¨æ¡ˆä¾‹
{slide.title}åœ¨å®é™…ä¸­æœ‰å¤šç§åº”ç”¨åœºæ™¯ã€‚ä¾‹å¦‚åœ¨æ–‡æœ¬åˆ†ç±»ã€åƒåœ¾é‚®ä»¶è¿‡æ»¤ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸéƒ½æœ‰æˆåŠŸåº”ç”¨æ¡ˆä¾‹ã€‚

## ğŸš€ å‰æ²¿è¿›å±•ä¸è¶‹åŠ¿
å½“å‰è¯¥é¢†åŸŸçš„ç ”ç©¶æ­£åœ¨å‘æ›´å¤æ‚çš„æ¨¡å‹å’Œæ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯å‘å±•ã€‚æ·±åº¦å­¦ä¹ ä¸å¤§æ¨¡å‹çš„ç»“åˆæ˜¯è¯¥é¢†åŸŸçš„å‰æ²¿æ–¹å‘ä¹‹ä¸€ã€‚

## ğŸ¯ æ·±å…¥å­¦ä¹ å»ºè®®
å»ºè®®é€šè¿‡ä»¥ä¸‹æ–¹å¼æ·±å…¥å­¦ä¹ {slide.title}ï¼š
1. é˜…è¯»ç›¸å…³æ•™ç§‘ä¹¦å’Œå­¦æœ¯è®ºæ–‡
2. å®è·µä»£ç å®ç°ï¼ŒåŠ æ·±ç†è§£
3. å‚ä¸åœ¨çº¿è¯¾ç¨‹å’Œç ”è®¨ä¼š
4. å…³æ³¨è¯¥é¢†åŸŸçš„æœ€æ–°ç ”ç©¶æˆæœ

## ğŸ”— æ¨èèµ„æº
- Wikipediaç›¸å…³æ¡ç›®
- æœºå™¨å­¦ä¹ ç»å…¸æ•™æ
- ç›¸å…³å­¦æœ¯ä¼šè®®è®ºæ–‡
- å¼€æºä»£ç åº“å’Œå·¥å…·"""

    def _create_error_extended_reading(self, slide: SlideContent, error: Exception) -> str:
        """åˆ›å»ºé”™è¯¯çŠ¶æ€ä¸‹çš„å»¶ä¼¸é˜…è¯»ææ–™"""
        return f"""# ã€Š{slide.title}ã€‹çŸ¥è¯†æ·±åº¦æ¢ç´¢

## âš ï¸ ç³»ç»Ÿæç¤º
ç”±äºç³»ç»Ÿé”™è¯¯ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´çš„çŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™ã€‚é”™è¯¯ä¿¡æ¯: {str(error)}

## ğŸ“ å»ºè®®æ–¹æ¡ˆ
æ‚¨å¯ä»¥ï¼š
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. ç¨åé‡è¯•
3. è”ç³»æŠ€æœ¯æ”¯æŒ

## ğŸ” æ›¿ä»£å­¦ä¹ èµ„æº
å»ºè®®å‚è€ƒä»¥ä¸‹èµ„æºå­¦ä¹ {slide.title}ï¼š
- ç›¸å…³æ•™ç§‘ä¹¦
- åœ¨çº¿è¯¾ç¨‹
- å­¦æœ¯è®ºæ–‡
- ä¸“ä¸šè®ºå›"""

    async def _generate_explanation_with_validation(self, slide: SlideContent) -> Tuple[
        List[Dict[str, str]], Dict[str, Any]]:
        """ç”Ÿæˆè¯¦ç»†è§£é‡Šï¼ˆå¸¦æ ¡éªŒï¼‰"""
        content_text = "\n".join(slide.content)
        bullet_text = "\n".join(slide.bullet_points) if slide.bullet_points else "æ— "

        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æ•™å¸ˆï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚æ¦‚å¿µã€‚è¯·æ ¹æ®PPTå¹»ç¯ç‰‡å†…å®¹ï¼Œæä¾›è¯¦ç»†çš„è§£é‡Šè¯´æ˜ã€‚è¯·ç¡®ä¿è§£é‡Šå‡†ç¡®ã€å®Œæ•´ï¼Œé¿å…ä¸ç¡®å®šçš„è¡¨è¿°ã€‚"
            },
            {
                "role": "user",
                "content": f"""å¹»ç¯ç‰‡å†…å®¹ï¼š

æ ‡é¢˜ï¼š{slide.title}

ä¸»è¦å†…å®¹ï¼š
{content_text}

é¡¹ç›®ç¬¦å·ï¼š
{bullet_text}

è¯·ä¸ºè¿™ä¸ªå¹»ç¯ç‰‡ç”Ÿæˆè¯¦ç»†çš„è§£é‡Šï¼ŒåŒ…æ‹¬ï¼š
1. æ ¸å¿ƒæ¦‚å¿µçš„å®šä¹‰å’Œè¯´æ˜
2. åŸºæœ¬åŸç†å’Œé€»è¾‘
3. å®é™…åº”ç”¨åœºæ™¯
4. ä¸å…¶ä»–çŸ¥è¯†çš„å…³è”

è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ï¼Œç¡®ä¿è§£é‡Šæ¸…æ™°å‡†ç¡®ã€‚è¯·ç›´æ¥å¼€å§‹è§£é‡Šï¼Œä¸è¦æ·»åŠ é¢å¤–çš„è¯´æ˜æ€§æ–‡å­—ã€‚"""
            }
        ]

        try:
            explanation, validation_result = await self.call_llm_with_validation(
                messages, task_type="explanation", expected_format="explanation"
            )

            if explanation and validation_result.get("passed", False):
                # ã€ä¿®å¤1ã€‘æ¸…ç†è§£é‡Šæ–‡æœ¬
                explanation_clean = explanation.strip()

                # ç§»é™¤å¼€å¤´å¯èƒ½å­˜åœ¨çš„é‡å¤æ ‡ç­¾
                remove_prefixes = [
                    "æ ¸å¿ƒæ¦‚å¿µï¼š", "æ ¸å¿ƒæ¦‚å¿µ:", "æ ¸å¿ƒæ¦‚å¿µ",
                    "æ¦‚å¿µï¼š", "æ¦‚å¿µ:", "æ¦‚å¿µ",
                    "ä¸€ã€æ ¸å¿ƒæ¦‚å¿µ", "1. æ ¸å¿ƒæ¦‚å¿µ",
                    "ã€æ ¸å¿ƒæ¦‚å¿µã€‘", "[æ ¸å¿ƒæ¦‚å¿µ]",
                ]

                for prefix in remove_prefixes:
                    if explanation_clean.startswith(prefix):
                        explanation_clean = explanation_clean[len(prefix):].strip()

                # ã€ä¿®å¤2ã€‘æ¸…ç†æ®µè½å¼€å¤´çš„é‡å¤æ ‡ç­¾
                lines = explanation_clean.split('\n')
                cleaned_lines = []

                for line in lines:
                    line_stripped = line.strip()
                    # è·³è¿‡ç©ºè¡Œ
                    if not line_stripped:
                        continue
                    # è·³è¿‡çº¯æ ‡ç­¾è¡Œ
                    if line_stripped in ["æ ¸å¿ƒæ¦‚å¿µ", "æ¦‚å¿µ", "ä¸€ã€æ ¸å¿ƒæ¦‚å¿µ", "1. æ ¸å¿ƒæ¦‚å¿µ"]:
                        continue
                    # ç§»é™¤è¡Œé¦–çš„æ ‡ç­¾
                    for prefix in remove_prefixes:
                        if line_stripped.startswith(prefix):
                            line_stripped = line_stripped[len(prefix):].strip()
                            break
                    cleaned_lines.append(line_stripped)

                explanation_clean = '\n\n'.join(cleaned_lines)

                # ã€ä¿®å¤3ã€‘æ™ºèƒ½åˆ†æ®µï¼ˆè€Œä¸æ˜¯ç®€å•æŒ‰ç©ºè¡Œåˆ†å‰²ï¼‰
                explanations = []

                # å¦‚æœè§£é‡Šè¾ƒçŸ­ï¼Œä½œä¸ºæ•´ä½“å¤„ç†
                if len(explanation_clean) < 500:
                    # å°è¯•æŒ‰è‡ªç„¶æ®µè½åˆ†å‰²
                    paragraphs = re.split(r'\n\s*\n+', explanation_clean)
                    if len(paragraphs) == 1:
                        # åªæœ‰ä¸€ä¸ªæ®µè½ï¼Œä½œä¸ºæ•´ä½“
                        explanations.append({
                            "concept": "è¯¦ç»†è§£é‡Š",  # â† ä½¿ç”¨å›ºå®šçš„ã€ä¸é‡å¤çš„æ¦‚å¿µå
                            "explanation": explanation_clean,
                            "type": "general_explanation",
                            "validation": validation_result
                        })
                    else:
                        # å¤šä¸ªæ®µè½ï¼Œä¸ºæ¯ä¸ªæ®µè½æ·»åŠ é€‚å½“çš„æ ‡é¢˜
                        for i, para in enumerate(paragraphs):
                            if para.strip():
                                concept_name = f"è§£é‡Šéƒ¨åˆ†{i + 1}"
                                if i == 0:
                                    concept_name = "æ¦‚è¿°"
                                elif i == len(paragraphs) - 1:
                                    concept_name = "æ€»ç»“"

                                explanations.append({
                                    "concept": concept_name,
                                    "explanation": para.strip(),
                                    "type": "general_explanation",
                                    "validation": validation_result
                                })
                else:
                    # è¾ƒé•¿çš„è§£é‡Šï¼Œå¯»æ‰¾è‡ªç„¶çš„å°æ ‡é¢˜
                    sections = []
                    current_section = []
                    current_title = "è¯¦ç»†è§£é‡Š"

                    lines = explanation_clean.split('\n')
                    for line in lines:
                        line_stripped = line.strip()
                        # æ£€æµ‹å¯èƒ½çš„å°æ ‡é¢˜ï¼ˆæ•°å­—æˆ–ä¸­æ–‡æ•°å­—å¼€å¤´ï¼‰
                        if re.match(r'^(?:\d+[\.ã€]|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]ã€?|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€)',
                                    line_stripped):
                            # ä¿å­˜å½“å‰section
                            if current_section:
                                sections.append({
                                    "title": current_title,
                                    "content": '\n'.join(current_section)
                                })
                            # å¼€å§‹æ–°çš„section
                            current_title = line_stripped
                            current_section = []
                        else:
                            current_section.append(line)

                    # æ·»åŠ æœ€åä¸€ä¸ªsection
                    if current_section:
                        sections.append({
                            "title": current_title,
                            "content": '\n'.join(current_section)
                        })

                    # è½¬æ¢ä¸ºexplanationsæ ¼å¼
                    for section in sections:
                        # æ¸…ç†æ ‡é¢˜
                        section_title = section["title"]
                        for prefix in remove_prefixes:
                            if section_title.startswith(prefix):
                                section_title = section_title[len(prefix):].strip()
                                break

                        explanations.append({
                            "concept": section_title if section_title else "è¯¦ç»†è§£é‡Š",
                            "explanation": section["content"].strip(),
                            "type": "general_explanation",
                            "validation": validation_result
                        })

                # ã€ä¿®å¤4ã€‘å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•è§£é‡Šï¼Œè¿”å›æ•´ä½“è§£é‡Š
                if not explanations:
                    explanations.append({
                        "concept": "è¯¦ç»†è§£é‡Š",  # â† é¿å…ä½¿ç”¨"æ ¸å¿ƒæ¦‚å¿µ"
                        "explanation": explanation_clean,
                        "type": "general_explanation",
                        "validation": validation_result
                    })

                return explanations, validation_result

            else:
                # å¦‚æœæ ¡éªŒå¤±è´¥ï¼Œè¿”å›é™çº§å¤„ç†çš„ç»“æœ
                fallback_explanation = self._create_fallback_explanation(slide)
                return [{
                    "concept": "åŸºæœ¬è§£é‡Š",
                    "explanation": fallback_explanation,
                    "type": "fallback_explanation",
                    "validation": validation_result,
                    "fallback_reason": "æ ¡éªŒæœªé€šè¿‡ï¼Œä½¿ç”¨é™çº§å¤„ç†"
                }], validation_result

        except Exception as e:
            logger.error(f"ç”Ÿæˆè§£é‡Šå¤±è´¥: {e}")
            return [], {"passed": False, "errors": [str(e)]}

    async def _generate_examples_with_validation(self, slide: SlideContent) -> Tuple[
        List[Dict[str, str]], Dict[str, Any]]:
        """ç”Ÿæˆä»£ç /åº”ç”¨ç¤ºä¾‹ï¼ˆå¸¦æ ¡éªŒï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯æŠ€æœ¯ç›¸å…³çš„å†…å®¹
        technical_keywords = ["ä»£ç ", "ç¼–ç¨‹", "ç®—æ³•", "å‡½æ•°", "ç±»", "å¯¹è±¡", "æ•°æ®åº“", "ç½‘ç»œ", "åè®®"]

        slide_text = slide.title + " ".join(slide.content) + " ".join(slide.bullet_points)
        is_technical = any(keyword in slide_text for keyword in technical_keywords)

        if not is_technical:
            return [], {"passed": True, "skipped": "éæŠ€æœ¯å†…å®¹"}

        content_text = "\n".join(slide.content)

        messages = [
            {"role": "system", "content": """ä½ æ˜¯ä¸€ä½èµ„æ·±ç¨‹åºå‘˜å’Œæ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿ç”¨æ ¸å¿ƒä»£ç ç¤ºä¾‹è§£é‡ŠæŠ€æœ¯æ¦‚å¿µã€‚

        ã€é‡è¦è¦æ±‚ã€‘ï¼š
        è¯·æä¾›**æ ¸å¿ƒåŠŸèƒ½ä»£ç ç¤ºä¾‹**ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„é¡¹ç›®ä»£ç ã€‚

        **æ ¸å¿ƒä»£ç ç¤ºä¾‹åº”è¯¥**ï¼š
        1. å±•ç¤ºå…³é”®ç®—æ³•ã€æ ¸å¿ƒå‡½æ•°ã€ä¸»è¦é€»è¾‘
        2. ä»£ç é•¿åº¦æ§åˆ¶åœ¨20-100è¡Œä»¥å†…
        3. åŒ…å«è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šè¯´æ˜
        4. æ ¸å¿ƒéƒ¨åˆ†å¯ä»¥ç›´æ¥è¿è¡Œå’Œç†è§£
        5. çªå‡ºå±•ç¤ºæŠ€æœ¯è¦ç‚¹

        **ä¸è¦æä¾›**ï¼š
        - å®Œæ•´çš„é¡¹ç›®ä»£ç 
        - æ‰€æœ‰é”™è¯¯å¤„ç†
        - å®Œæ•´çš„æµ‹è¯•å¥—ä»¶
        - é…ç½®æ–‡ä»¶å’Œä¾èµ–ç®¡ç†
        - æ€§èƒ½ä¼˜åŒ–ç»†èŠ‚

        ä¸“æ³¨äºç”Ÿæˆç²¾ç®€ã€å¯ç†è§£çš„æ ¸å¿ƒä»£ç ç¤ºä¾‹ã€‚"""},
            {"role": "user", "content": f"""è¯·æ ¹æ®ä»¥ä¸‹å¹»ç¯ç‰‡å†…å®¹ï¼Œç”Ÿæˆ**æ ¸å¿ƒåŠŸèƒ½ä»£ç ç¤ºä¾‹**ï¼š

        ä¸»é¢˜ï¼š{slide.title}
        å†…å®¹ï¼š{content_text}

        è¯·æä¾›ï¼š
        1. æ ¸å¿ƒåŠŸèƒ½çš„ä»£ç ç¤ºä¾‹ï¼ˆ20-100è¡Œï¼‰ï¼Œä»¥```pythonå¼€å¤´ï¼Œ```ç»“å°¾
        2. ä¸­æ–‡æ³¨é‡Šè¯´æ˜å…³é”®éƒ¨åˆ†
        3. ç®€è¦çš„è¿è¡Œç»“æœè¯´æ˜

        è¯·ä½¿ç”¨Pythonè¯­è¨€ï¼Œå±•ç¤ºæœ€æ ¸å¿ƒçš„å®ç°é€»è¾‘ã€‚
        è¯·ç¡®ä¿ä»£ç è¯­æ³•æ­£ç¡®ï¼Œæ ¸å¿ƒéƒ¨åˆ†å¯è¿è¡Œã€‚

        **ã€æ³¨æ„ã€‘åªç”Ÿæˆæ ¸å¿ƒä»£ç ï¼Œä¸è¦ç”Ÿæˆå®Œæ•´çš„é•¿ç¯‡ä»£ç ï¼**"""}
        ]

        try:
            example, validation_result = await self.call_llm_with_validation(
                messages, task_type="code_example", expected_format="code_example"
            )

            if example and validation_result.get("passed", False):
                # è¿›ä¸€æ­¥æ ¡éªŒä»£ç è¯­æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
                if self._check_python_syntax_basic(example):
                    return [{
                        "language": "python",
                        "code_example": example,
                        "description": "åŸºäºå¹»ç¯ç‰‡å†…å®¹ç”Ÿæˆçš„ä»£ç ç¤ºä¾‹",
                        "type": "code_example",
                        "validation": validation_result
                    }], validation_result
                else:
                    validation_result["warnings"].append("ä»£ç è¯­æ³•æ£€æŸ¥æœªé€šè¿‡")
                    return [], validation_result
            else:
                return [], validation_result

        except Exception as e:
            logger.error(f"ç”Ÿæˆç¤ºä¾‹å¤±è´¥: {e}")
            return [], {"passed": False, "errors": [str(e)]}

    async def _fetch_wikipedia_articles(self, keyword: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """è°ƒç”¨Wikipedia APIè·å–ç›¸å…³æ¡ç›® - ä¿®å¤ç‰ˆ"""
        try:
            logger.info(f"å¼€å§‹Wikipediaæœç´¢: '{keyword}'")

            # ã€ä¿®å¤1ã€‘ä½¿ç”¨è‹±æ–‡ç»´åŸºç™¾ç§‘APIï¼ˆå¯¹è‹±æ–‡å…³é”®è¯å“åº”æ›´å¥½ï¼‰
            api_url = "https://en.wikipedia.org/w/api.php"

            # ã€ä¿®å¤2ã€‘å°è¯•ç¿»è¯‘ä¸­æ–‡å…³é”®è¯ä¸ºè‹±æ–‡
            search_keyword = keyword
            if keyword in self.term_mapping:
                search_keyword = self.term_mapping[keyword]
                logger.info(f"å…³é”®è¯ç¿»è¯‘: '{keyword}' -> '{search_keyword}'")

            # ã€ä¿®å¤3ã€‘ä½¿ç”¨requestsåŒæ­¥è°ƒç”¨ï¼ˆå·²å¯¼å…¥ï¼‰
            params = {
                "action": "query",
                "format": "json",
                "list": "search",
                "srsearch": search_keyword,
                "srlimit": max_results,
                "srprop": "snippet|size|wordcount",
                "srwhat": "text",
                "utf8": 1,
                "origin": "*"
            }

            headers = {
                "User-Agent": "PPT-Extension-Agent/1.0 (https://github.com/edu-tool; contact@example.com)"
            }

            # ä½¿ç”¨requestsè¿›è¡ŒåŒæ­¥è°ƒç”¨
            response = requests.get(
                api_url,
                params=params,
                headers=headers,
                timeout=10,
                verify=False
            )

            logger.info(f"Wikipediaå“åº”çŠ¶æ€: {response.status_code}")

            if response.status_code == 200:
                data = response.json()

                if "query" in data and "search" in data["query"]:
                    articles = data["query"]["search"]
                    logger.info(f"âœ… æˆåŠŸè·å– {len(articles)} ä¸ªç»“æœï¼Œå…³é”®è¯: {search_keyword}")

                    processed_articles = []
                    for article in articles:
                        title = article.get("title", "")
                        snippet = article.get("snippet", "")

                        # æ¸…ç†HTMLæ ‡ç­¾
                        if snippet:
                            snippet = re.sub(r'<[^>]+>', '', snippet)
                            snippet = re.sub(r'&\w+;', '', snippet)
                            snippet = snippet.replace('\n', ' ').strip()

                        # æ„å»ºURL
                        url = f"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}"

                        processed_articles.append({
                            "title": title,
                            "description": snippet[:200] + "..." if snippet and len(snippet) > 200 else snippet or "",
                            "url": url,
                            "pageid": article.get("pageid", ""),
                            "wordcount": article.get("wordcount", 0),
                            "original_keyword": keyword,
                            "search_keyword": search_keyword
                        })

                    return processed_articles
                else:
                    logger.warning(f"Wikipediaå“åº”æ ¼å¼å¼‚å¸¸ï¼Œå…³é”®è¯: {search_keyword}")
                    if "error" in data:
                        logger.error(f"Wikipedia APIé”™è¯¯: {data['error']}")
                    return []
            else:
                text = response.text[:500]
                logger.error(f"Wikipedia HTTPé”™è¯¯ {response.status_code}ï¼Œå…³é”®è¯: {search_keyword}: {text}")
                return []

        except requests.exceptions.Timeout:
            logger.error(f"Wikipediaè¯·æ±‚è¶…æ—¶ (10ç§’)ï¼Œå…³é”®è¯: {keyword}")
            return []
        except requests.exceptions.SSLError as e:
            logger.error(f"Wikipedia SSLé”™è¯¯ï¼Œå…³é”®è¯: {keyword}: {e}")
            return []
        except requests.exceptions.ConnectionError as e:
            logger.error(f"Wikipediaè¿æ¥é”™è¯¯ï¼Œå…³é”®è¯: {keyword}: {e}")
            return []
        except Exception as e:
            logger.error(f"Wikipediaè¯·æ±‚å¼‚å¸¸ï¼Œå…³é”®è¯: {keyword}: {type(e).__name__}: {e}")
            return []

    async def _generate_extended_reading_with_validation(self, slide: SlideContent) -> Tuple[
        List[Dict[str, Any]], Dict[str, Any]]:
        """ç”Ÿæˆå»¶ä¼¸é˜…è¯»ææ–™ï¼ˆå¸¦æ ¡éªŒï¼‰- ç‹¬ç«‹çš„æ‰©å±•ä»»åŠ¡"""

        # æå–å…³é”®è¯ç”¨äºæœç´¢Wikipedia
        keywords = self._extract_keywords_from_slide(slide)

        if not keywords:
            logger.info(f"å¹»ç¯ç‰‡ {slide.slide_number} æ— æœ‰æ•ˆå…³é”®è¯ï¼Œç”ŸæˆåŸºç¡€å»¶ä¼¸é˜…è¯»")
            return await self._generate_basic_extended_reading(slide)

        # ä½¿ç”¨ä¸»å…³é”®è¯
        main_keyword = keywords[0]

        try:
            logger.info(f"å¼€å§‹ä¸ºå¹»ç¯ç‰‡ {slide.slide_number} ç”Ÿæˆå»¶ä¼¸é˜…è¯»ææ–™ï¼Œå…³é”®è¯: {main_keyword}")

            # 1. çœŸå®è°ƒç”¨Wikipedia APIè·å–æƒå¨æ¡ç›®
            wikipedia_articles = []

            try:
                # ã€ä¿®å¤ã€‘æ­£ç¡®è°ƒç”¨Wikipedia API
                wikipedia_articles = await self._fetch_wikipedia_articles(main_keyword)
                logger.info(f"Wikipediaè¯·æ±‚å®Œæˆï¼Œè·å– {len(wikipedia_articles)} ä¸ªç»“æœ")

                # å¦‚æœæ²¡æœ‰ç»“æœï¼Œå°è¯•å…¶ä»–å…³é”®è¯
                if not wikipedia_articles and len(keywords) > 1:
                    logger.info(f"ä¸»å…³é”®è¯æ— ç»“æœï¼Œå°è¯•å¤‡ç”¨å…³é”®è¯: {keywords[1]}")
                    wikipedia_articles = await self._fetch_wikipedia_articles(keywords[1])

            except Exception as e:
                logger.warning(f"Wikipediaè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡: {e}")
                wikipedia_articles = []

            # 2. åŸºäºWikipediaæƒå¨å†…å®¹ç”Ÿæˆå»¶ä¼¸é˜…è¯»ææ–™
            messages = [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä½æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿åŸºäºWikipediaæƒå¨å†…å®¹ç”Ÿæˆè¯¦ç»†çš„çŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™ã€‚
è¯·ç¡®ä¿çŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™åŸºäºçœŸå®ä¿¡æ¯ï¼Œå†…å®¹è¯¦å®ã€æœ‰ä»·å€¼ï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
1. çŸ¥è¯†æ·±åº¦æ‰©å±•
2. å†å²èƒŒæ™¯ä¸å‘å±•
3. å®é™…åº”ç”¨æ¡ˆä¾‹
4. å‰æ²¿è¿›å±•ä¸è¶‹åŠ¿
5. æ·±å…¥å­¦ä¹ å»ºè®®

æ¯ä¸ªéƒ¨åˆ†è¯·æä¾›300-500å­—çš„è¯¦ç»†å†…å®¹ï¼Œæ€»å­—æ•°åœ¨1500-2500å­—ä¹‹é—´ã€‚"""
                },
                {
                    "role": "user",
                    "content": f"""åŸºäºä»¥ä¸‹å¹»ç¯ç‰‡å†…å®¹ï¼Œç”Ÿæˆè¯¦ç»†çš„çŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™ï¼š

å¹»ç¯ç‰‡ä¸»é¢˜ï¼š{slide.title}
å¹»ç¯ç‰‡å†…å®¹ï¼š{' '.join(slide.content[:3]) if slide.content else 'æ— è¯¦ç»†å†…å®¹'}

{'ä»¥ä¸‹æ˜¯ç›¸å…³çš„Wikipediaæƒå¨æ¡ç›®ï¼Œè¯·åŸºäºè¿™äº›çœŸå®ä¿¡æ¯ç”Ÿæˆå†…å®¹ï¼š' + chr(10).join([f"ã€{article['title']}ã€‘{article['description'][:150]}..." for article in wikipedia_articles]) if wikipedia_articles else 'è¯·åŸºäºä¸“ä¸šçŸ¥è¯†ç”Ÿæˆç›¸å…³å†…å®¹ï¼š'}

è¯·ç”Ÿæˆè¯¦ç»†çš„çŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™ï¼Œè¦æ±‚ï¼š
1. å†…å®¹è¯¦å®ã€å‡†ç¡®ï¼Œç»“æ„æ¸…æ™°
2. åŒ…å«ä¸Šè¿°5ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†æœ‰è¯¦ç»†é˜è¿°
3. è¯­è¨€é€šä¿—æ˜“æ‡‚ï¼Œé€‚åˆå­¦ä¹ è€…é˜…è¯»
4. æä¾›æ·±å…¥çš„å­¦ä¹ å»ºè®®å’Œèµ„æºæ¨è
5. åœ¨æŠ¥å‘Šä¸­å‘½åä¸º"çŸ¥è¯†æ·±åº¦æ¢ç´¢"

è¯·ç›´æ¥å¼€å§‹ç”Ÿæˆå†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–çš„è¯´æ˜æ€§æ–‡å­—ã€‚"""
                }
            ]

            extended_reading_text, validation_result = await self.call_llm_with_validation(
                messages, task_type="extended_reading", expected_format="extended_reading"
            )

            if extended_reading_text and validation_result.get("passed", False):
                # æ·»åŠ Wikipediaæ¥æºä¿¡æ¯
                wikipedia_sources = []
                for article in wikipedia_articles:
                    wikipedia_sources.append({
                        "title": article["title"],
                        "url": article["url"],
                        "description": article["description"],
                        "original_keyword": article.get("original_keyword", "")
                    })

                # è§£æç« èŠ‚
                sections = self._parse_extended_reading_sections(extended_reading_text)

                # åˆ›å»ºå»¶ä¼¸é˜…è¯»ææ–™ - ä½¿ç”¨æ–°åç§°ã€çŸ¥è¯†æ·±åº¦æ¢ç´¢ã€‘
                extended_reading = [{
                    "title": f"ã€Š{slide.title}ã€‹çŸ¥è¯†æ·±åº¦æ¢ç´¢",  # âœ… ä½¿ç”¨æ–°åç§°
                    "content": extended_reading_text,
                    "sections": sections,
                    "wikipedia_sources": wikipedia_sources,
                    "total_length": len(extended_reading_text),
                    "section_count": len(sections),
                    "type": "extended_reading",
                    "validation": validation_result,
                    "source": "åŸºäºWikipedia APIç”Ÿæˆ" if wikipedia_articles else "LLMç”Ÿæˆ",
                    "display_name": "çŸ¥è¯†æ·±åº¦æ¢ç´¢"  # âœ… åœ¨æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„åç§°
                }]

                logger.info(f"âœ… æˆåŠŸç”ŸæˆçŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™ï¼Œé•¿åº¦: {len(extended_reading_text)}å­—ï¼Œç« èŠ‚æ•°: {len(sections)}")
                return extended_reading, validation_result
            else:
                logger.warning(f"çŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™æ ¡éªŒæœªé€šè¿‡ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼")
                # æ ¡éªŒå¤±è´¥ï¼Œé™çº§å¤„ç†
                return await self._generate_basic_extended_reading(slide)

        except Exception as e:
            logger.error(f"ç”ŸæˆçŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™å¤±è´¥: {e}", exc_info=True)
            return await self._generate_basic_extended_reading(slide)

    async def _generate_basic_extended_reading(self, slide: SlideContent) -> Tuple[
        List[Dict[str, Any]], Dict[str, Any]]:
        """ç”ŸæˆåŸºç¡€å»¶ä¼¸é˜…è¯»ææ–™"""

        logger.info(f"ä¸ºåŸºç¡€æ¨¡å¼ä¸ºå¹»ç¯ç‰‡ {slide.slide_number} ç”ŸæˆçŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™")

        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä½æ•™è‚²ä¸“å®¶ï¼Œä¸ºå­¦ä¹ ææ–™ç”ŸæˆçŸ¥è¯†æ·±åº¦æ¢ç´¢å†…å®¹ã€‚
è¯·ç¡®ä¿å†…å®¹è¯¦å®ã€æœ‰ä»·å€¼ï¼Œç»“æ„æ¸…æ™°ã€‚åœ¨æŠ¥å‘Šä¸­å‘½åä¸º"çŸ¥è¯†æ·±åº¦æ¢ç´¢"ã€‚"""
            },
            {
                "role": "user",
                "content": f"""ä¸ºä»¥ä¸‹å¹»ç¯ç‰‡ä¸»é¢˜ç”ŸæˆçŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™ï¼š

ä¸»é¢˜ï¼š{slide.title}
å†…å®¹æ‘˜è¦ï¼š{' '.join(slide.content[:2]) if slide.content else 'æ— è¯¦ç»†å†…å®¹'}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹éƒ¨åˆ†çš„çŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™ï¼š
1. çŸ¥è¯†æ·±åº¦æ‰©å±•ï¼šæ·±å…¥è§£é‡Šæ ¸å¿ƒæ¦‚å¿µå’ŒåŸç†
2. ç›¸å…³èƒŒæ™¯çŸ¥è¯†ï¼šä»‹ç»å†å²èƒŒæ™¯å’Œå‘å±•å†ç¨‹
3. å®é™…åº”ç”¨åœºæ™¯ï¼šåˆ—ä¸¾å…·ä½“åº”ç”¨æ¡ˆä¾‹
4. è¿›ä¸€æ­¥å­¦ä¹ å»ºè®®ï¼šæä¾›å­¦ä¹ è·¯å¾„å’Œèµ„æºå»ºè®®

æ¯ä¸ªéƒ¨åˆ†è¯·æä¾›300-500å­—çš„è¯¦ç»†å†…å®¹ã€‚è¯·ç›´æ¥å¼€å§‹ç”Ÿæˆå†…å®¹ã€‚"""
            }
        ]

        try:
            content, validation_result = await self.call_llm_with_validation(
                messages, task_type="extended_reading", expected_format="extended_reading"
            )

            if content and validation_result.get("passed", False):
                validation_result["warnings"] = validation_result.get("warnings", []) + ["ä½¿ç”¨åŸºç¡€æ¨¡å¼ç”Ÿæˆ"]

                # è§£æç« èŠ‚
                sections = self._parse_extended_reading_sections(content)

                return [{
                    "title": f"ã€Š{slide.title}ã€‹çŸ¥è¯†æ·±åº¦æ¢ç´¢",  # âœ… ä½¿ç”¨æ–°åç§°
                    "content": content,
                    "sections": sections,
                    "total_length": len(content),
                    "section_count": len(sections),
                    "type": "extended_reading_basic",
                    "validation": validation_result,
                    "source": "LLMåŸºç¡€ç”Ÿæˆ",
                    "display_name": "çŸ¥è¯†æ·±åº¦æ¢ç´¢"  # âœ… åœ¨æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„åç§°
                }], validation_result
            else:
                logger.warning(f"åŸºç¡€çŸ¥è¯†æ·±åº¦æ¢ç´¢ç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                return [], {"passed": False, "errors": ["æ— æ³•ç”ŸæˆçŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™"], "warnings": ["åŸºç¡€æ¨¡å¼ä¹Ÿå¤±è´¥"]}

        except Exception as e:
            logger.error(f"åŸºç¡€çŸ¥è¯†æ·±åº¦æ¢ç´¢ç”Ÿæˆå¤±è´¥: {e}")
            return [], {"passed": False, "errors": [str(e)]}

    def _parse_extended_reading_sections(self, content: str) -> List[Dict[str, str]]:
        """è§£æå»¶ä¼¸é˜…è¯»ææ–™çš„ç« èŠ‚"""
        sections = []

        # å°è¯•æŒ‰å¸¸è§ç« èŠ‚æ ‡é¢˜åˆ†å‰²
        section_patterns = [
            (r'(ä¸€ã€.*?)(?=äºŒã€|$)', 'ä¸€ã€'),
            (r'(äºŒã€.*?)(?=ä¸‰ã€|$)', 'äºŒã€'),
            (r'(ä¸‰ã€.*?)(?=å››ã€|$)', 'ä¸‰ã€'),
            (r'(å››ã€.*?)(?=äº”ã€|$)', 'å››ã€'),
            (r'(äº”ã€.*?)(?=å…­ã€|$)', 'äº”ã€'),
            (r'(##\s*çŸ¥è¯†.*?)(?=##\s*|$)', 'çŸ¥è¯†'),
            (r'(##\s*å†å².*?)(?=##\s*|$)', 'å†å²'),
            (r'(##\s*åº”ç”¨.*?)(?=##\s*|$)', 'åº”ç”¨'),
            (r'(##\s*å‰æ²¿.*?)(?=##\s*|$)', 'å‰æ²¿'),
            (r'(##\s*å­¦ä¹ .*?)(?=##\s*|$)', 'å­¦ä¹ '),
        ]

        # é¦–å…ˆå°è¯•æŒ‰æ¨¡å¼åˆ†å‰²
        for pattern, prefix in section_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            if matches and len(matches) >= 1:
                for match in matches:
                    match = match.strip()
                    if match:
                        # æå–æ ‡é¢˜å’Œå†…å®¹
                        lines = match.split('\n')
                        title = lines[0].strip()
                        section_content = '\n'.join(lines[1:]).strip()
                        if section_content:
                            sections.append({
                                'title': title,
                                'content': section_content
                            })
                if sections and len(sections) >= 2:
                    return sections

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼ŒæŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        section_titles = ["çŸ¥è¯†æ·±åº¦æ‰©å±•", "å†å²èƒŒæ™¯ä¸å‘å±•", "å®é™…åº”ç”¨æ¡ˆä¾‹", "å‰æ²¿è¿›å±•ä¸è¶‹åŠ¿", "æ·±å…¥å­¦ä¹ å»ºè®®"]

        for i, para in enumerate(paragraphs[:5]):  # æœ€å¤š5ä¸ªéƒ¨åˆ†
            title = section_titles[i] if i < len(section_titles) else f"ç¬¬{i + 1}éƒ¨åˆ†"
            sections.append({
                'title': title,
                'content': para
            })

        return sections

    async def _generate_references_with_validation(self, slide: SlideContent) -> Tuple[
        List[Dict[str, str]], Dict[str, Any]]:
        """ç”Ÿæˆå‚è€ƒèµ„æ–™ï¼ˆå¸¦æ ¡éªŒï¼‰- ä¸»è¦åŒ…å«Wikipediaå’Œå…¶ä»–æƒå¨èµ„æº"""

        # æå–å…³é”®è¯
        keywords = self._extract_keywords_from_slide(slide)

        if not keywords:
            return [], {"passed": True, "skipped": "æ— æœ‰æ•ˆå…³é”®è¯"}

        try:
            # ä½¿ç”¨æœ€ä½³å…³é”®è¯
            best_keyword = keywords[0]
            logger.info(f"ç”Ÿæˆå‚è€ƒèµ„æ–™ï¼Œä½¿ç”¨æœ€ä½³å…³é”®è¯: {best_keyword}")

            # è·å–Wikipediaèµ„æº
            wikipedia_resources = []

            try:
                wikipedia_resources = await self._fetch_wikipedia_articles(best_keyword)
                logger.info(f"Wikipediaè¯·æ±‚å®Œæˆï¼Œè·å– {len(wikipedia_resources)} ä¸ªç»“æœ")
            except Exception as e:
                logger.warning(f"Wikipediaè¯·æ±‚å¼‚å¸¸ï¼Œå…³é”®è¯: {best_keyword}: {e}")
                wikipedia_resources = []

            # å¦‚æœæ²¡æœ‰ç»“æœï¼Œå°è¯•ç¬¬äºŒä¸ªå…³é”®è¯
            if not wikipedia_resources and len(keywords) > 1:
                logger.info(f"ä¸»å…³é”®è¯æ— ç»“æœï¼Œå°è¯•å¤‡ç”¨å…³é”®è¯: {keywords[1]}")
                try:
                    wikipedia_resources = await self._fetch_wikipedia_articles(keywords[1])
                    logger.info(f"å¤‡ç”¨å…³é”®è¯è¯·æ±‚å®Œæˆï¼Œè·å– {len(wikipedia_resources)} ä¸ªç»“æœ")
                except Exception as e:
                    logger.warning(f"å¤‡ç”¨å…³é”®è¯è¯·æ±‚å¤±è´¥: {e}")
                    wikipedia_resources = []

            # è®¾ç½®æœ€å°å†…å®¹é•¿åº¦ï¼Œè¿‡æ»¤æ— ç”¨ç»“æœ
            filtered_resources = []
            for resource in wikipedia_resources:
                description = resource.get('description', '')
                if len(description) > 10:  # è‡³å°‘10ä¸ªå­—ç¬¦
                    filtered_resources.append(resource)

            # å»é‡
            unique_resources = {}
            for resource in filtered_resources:
                unique_resources[resource['title']] = resource

            wikipedia_resources = list(unique_resources.values())[:3]  # æœ€å¤š3ä¸ª

            # å¦‚æœWikipediaæ²¡æœ‰ç»“æœï¼Œç›´æ¥ç”ŸæˆLLMæ¨èçš„èµ„æº
            if not wikipedia_resources:
                logger.info(f"Wikipedia APIæ— ç»“æœï¼Œç›´æ¥ç”ŸæˆLLMæ¨èèµ„æº")
                other_resources = await self._generate_other_resources_simple(slide)
                all_resources = other_resources
            else:
                # ç”Ÿæˆå…¶ä»–èµ„æºæ¨è
                other_resources = await self._generate_other_resources(slide, wikipedia_resources)
                all_resources = wikipedia_resources + other_resources

            # åˆ›å»ºéªŒè¯ç»“æœ
            validation_result = {
                "passed": len(all_resources) > 0,
                "errors": [],
                "warnings": [],
                "retries": 0,
                "wikipedia_count": len(wikipedia_resources),
                "other_count": len(other_resources),
                "validation_details": {
                    "wikipedia_api_called": True,
                    "total_resources": len(all_resources)
                }
            }

            if not wikipedia_resources:
                validation_result["warnings"].append("Wikipedia APIæœªè¿”å›ç»“æœ")

            # æ ¼å¼åŒ–ç»“æœ
            formatted_resources = []
            for resource in all_resources:
                formatted_resources.append({
                    "title": resource.get("title", ""),
                    "description": resource.get("description", ""),
                    "url": resource.get("url", ""),
                    "type": resource.get("type", "reference"),
                    "source": resource.get("source", "unknown"),
                    "validation": validation_result
                })

            logger.info(f"âœ… ç”Ÿæˆå‚è€ƒèµ„æ–™ {len(formatted_resources)} ä¸ª")
            return formatted_resources, validation_result

        except Exception as e:
            logger.error(f"ç”Ÿæˆå‚è€ƒèµ„æ–™å¤±è´¥: {e}", exc_info=True)
            return [], {"passed": False, "errors": [str(e)]}

    async def _generate_other_resources_simple(self, slide: SlideContent) -> List[Dict[str, Any]]:
        """ç®€åŒ–ç‰ˆçš„å…¶ä»–èµ„æºç”Ÿæˆï¼ˆå½“Wikipedia APIæ— ç»“æœæ—¶ä½¿ç”¨ï¼‰"""
        try:
            messages = [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä½æ•™è‚²ä¸“å®¶ï¼ŒåŸºäºå­¦ä¹ ä¸»é¢˜æ¨èç›¸å…³çš„å­¦ä¹ èµ„æºã€‚
è¯·ç¡®ä¿æ¨èçœŸå®å­˜åœ¨çš„èµ„æºï¼ŒåŒ…æ‹¬ä¹¦ç±ã€è¯¾ç¨‹ã€è®ºæ–‡ç­‰ã€‚"""
                },
                {
                    "role": "user",
                    "content": f"""åŸºäºä»¥ä¸‹å­¦ä¹ ä¸»é¢˜ï¼Œæ¨èç›¸å…³çš„å­¦ä¹ èµ„æºï¼š

ä¸»é¢˜ï¼š{slide.title}
å†…å®¹æ‘˜è¦ï¼š{' '.join(slide.content[:2]) if slide.content else 'æ— è¯¦ç»†å†…å®¹'}

è¯·æ¨è3-4ä¸ªç›¸å…³çš„å­¦ä¹ èµ„æºï¼ŒåŒ…æ‹¬ï¼š
1. ç»å…¸ä¹¦ç±
2. åœ¨çº¿è¯¾ç¨‹æˆ–æ•™ç¨‹
3. é‡è¦çš„å­¦æœ¯è®ºæ–‡æˆ–ç ”ç©¶
4. å®ç”¨çš„å·¥å…·æˆ–æ¡†æ¶

å¯¹äºæ¯ä¸ªæ¨èï¼Œè¯·æä¾›ï¼š
- èµ„æºåç§°
- ç®€è¦æè¿°ï¼ˆ30å­—å†…ï¼‰
- æ¨èç†ç”±ï¼ˆ20å­—å†…ï¼‰

è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ï¼Œæ ¼å¼ä¸ºï¼šèµ„æºåç§° | æè¿° | æ¨èç†ç”±"""
                }
            ]

            resources_text, validation_result = await self.call_llm_with_validation(
                messages, task_type="references", expected_format=None
            )

            if resources_text and validation_result.get("passed", False):
                parsed_resources = []
                lines = resources_text.split("\n")

                for line in lines:
                    line = line.strip()
                    if line and '|' in line and len(line.split('|')) >= 2:
                        parts = line.split('|')
                        if len(parts) >= 2:
                            resource_name = parts[0].strip()
                            description = parts[1].strip()
                            reason = parts[2].strip() if len(parts) > 2 else "ç›¸å…³å­¦ä¹ èµ„æº"

                            parsed_resources.append({
                                "title": resource_name,
                                "description": description,
                                "type": self._determine_resource_type(resource_name),
                                "source": "LLMæ¨èï¼ˆç®€åŒ–ç‰ˆï¼‰",
                                "reason": reason
                            })

                return parsed_resources[:3]  # æœ€å¤šè¿”å›3ä¸ª

            return []

        except Exception as e:
            logger.error(f"ç”Ÿæˆç®€åŒ–ç‰ˆå…¶ä»–èµ„æºå¤±è´¥: {e}")
            return []

    async def _fetch_wikipedia_page_info(self, title: str) -> Dict[str, Any]:
        """è·å–Wikipediaé¡µé¢çš„æ›´å¤šä¿¡æ¯"""
        try:
            encoded_title = urllib.parse.quote(title)
            url = f"https://en.wikipedia.org/w/api.php"  # ã€ä¿®å¤ã€‘ä½¿ç”¨è‹±æ–‡ç»´åŸº

            params = {
                "action": "query",
                "format": "json",
                "prop": "extracts|info",
                "titles": encoded_title,
                "exintro": 1,
                "explaintext": 1,
                "inprop": "url",
                "utf8": 1,
                "origin": "*"
            }

            async with aiohttp.ClientSession() as session:
                headers = {
                    "User-Agent": "PPT-Agent/1.0 (https://github.com/your-repo; your-email@example.com) Education-Tool"
                }
                async with session.get(url, params=params, headers=headers, timeout=60.0) as response:
                    if response.status == 200:
                        data = await response.json()
                        if "query" in data and "pages" in data["query"]:
                            pages = data["query"]["pages"]
                            for page_id, page_info in pages.items():
                                if page_id != "-1":  # æœ‰æ•ˆé¡µé¢
                                    return {
                                        "extract": page_info.get("extract", "")[:300],
                                        "fullurl": page_info.get("fullurl", ""),
                                        "pageid": page_id,
                                        "touched": page_info.get("touched", "")
                                    }
            return {}
        except Exception:
            return {}

    async def _generate_other_resources(self, slide: SlideContent, wikipedia_resources: List[Dict]) -> List[
        Dict[str, Any]]:
        """ç”Ÿæˆå…¶ä»–ç±»å‹çš„å­¦ä¹ èµ„æº"""

        if not wikipedia_resources:
            return []

        # åŸºäºWikipediaç»“æœç”Ÿæˆå…¶ä»–èµ„æºæ¨è
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä½æ•™è‚²ä¸“å®¶ï¼ŒåŸºäºWikipediaæ¡ç›®æ¨èå…¶ä»–å­¦ä¹ èµ„æºã€‚
è¯·ç¡®ä¿æ¨èçœŸå®å­˜åœ¨çš„èµ„æºï¼ŒåŒ…æ‹¬ä¹¦ç±ã€è¯¾ç¨‹ã€è®ºæ–‡ç­‰ã€‚"""
            },
            {
                "role": "user",
                "content": f"""åŸºäºä»¥ä¸‹Wikipediaæ¡ç›®ï¼Œæ¨èç›¸å…³çš„å…¶ä»–å­¦ä¹ èµ„æºï¼š

ä¸»é¢˜ï¼š{slide.title}
Wikipediaæ¡ç›®ï¼š
{chr(10).join([f"- {item['title']}: {item['description'][:100]}..." for item in wikipedia_resources])}

è¯·æ¨èï¼š
1. ç›¸å…³çš„ç»å…¸ä¹¦ç±ï¼ˆçœŸå®å­˜åœ¨çš„ï¼‰
2. ä¼˜è´¨çš„åœ¨çº¿è¯¾ç¨‹æˆ–æ•™ç¨‹
3. é‡è¦çš„å­¦æœ¯è®ºæ–‡
4. å®ç”¨çš„å·¥å…·æˆ–æ¡†æ¶

å¯¹äºæ¯ä¸ªæ¨èï¼Œè¯·æä¾›ï¼š
- èµ„æºåç§°
- ç®€è¦æè¿°ï¼ˆ50å­—å†…ï¼‰
- æ¨èç†ç”±

æ ¼å¼ï¼š
èµ„æºåç§° | æè¿° | æ¨èç†ç”±"""
            }
        ]

        try:
            resources_text, validation_result = await self.call_llm_with_validation(
                messages, task_type="references", expected_format=None
            )

            if resources_text and validation_result.get("passed", False):
                parsed_resources = []
                lines = resources_text.split("\n")

                for line in lines:
                    line = line.strip()
                    if line and '|' in line and len(line.split('|')) >= 2:
                        parts = line.split('|')
                        if len(parts) >= 2:
                            resource_name = parts[0].strip()
                            description = parts[1].strip()
                            reason = parts[2].strip() if len(parts) > 2 else "ç›¸å…³å­¦ä¹ èµ„æº"

                            parsed_resources.append({
                                "title": resource_name,
                                "description": description,
                                "type": self._determine_resource_type(resource_name),
                                "source": "LLMæ¨è",
                                "reason": reason
                            })

                return parsed_resources[:4]  # æœ€å¤šè¿”å›4ä¸ª

            return []

        except Exception as e:
            logger.error(f"ç”Ÿæˆå…¶ä»–èµ„æºå¤±è´¥: {e}")
            return []

    async def _generate_quiz_with_validation(self, slide: SlideContent) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """ç”Ÿæˆæµ‹éªŒé—®é¢˜ï¼ˆå¸¦æ ¡éªŒï¼‰"""
        content_text = "\n".join(slide.content)

        messages = [
            {"role": "system", "content": """ä½ æ˜¯ä¸€ä½è€ƒè¯•å‘½é¢˜ä¸“å®¶ï¼Œæ“…é•¿è®¾è®¡æµ‹è¯•å­¦ç”Ÿç†è§£ç¨‹åº¦çš„é—®é¢˜ã€‚
        è¯·ç¡®ä¿é—®é¢˜è®¾è®¡åˆç†ï¼Œç­”æ¡ˆå‡†ç¡®æ— è¯¯ï¼Œè§£ææ¸…æ™°æ˜äº†ã€‚

        ã€é‡è¦è¦æ±‚ã€‘ï¼š
        1. é—®é¢˜å¿…é¡»åŸºäºæä¾›çš„å­¦ä¹ å†…å®¹
        2. å¿…é¡»æä¾›è¯¦ç»†çš„ç­”æ¡ˆè§£æï¼Œä¸èƒ½ä¸ºç©º
        3. è§£æè¦è§£é‡Šä¸ºä»€ä¹ˆæ­£ç¡®é€‰é¡¹æ­£ç¡®ï¼Œä»¥åŠå…¶ä»–é€‰é¡¹ä¸ºä»€ä¹ˆä¸æ­£ç¡®

        è§£ææ ¼å¼è¦æ±‚ï¼š
        1. é¦–å…ˆè¯´æ˜æ­£ç¡®ç­”æ¡ˆçš„æ­£ç¡®æ€§åŸå› 
        2. ç®€è¦åˆ†æå…¶ä»–é€‰é¡¹çš„é”™è¯¯åŸå› 
        3. å¯ä»¥åŒ…å«ç›¸å…³çŸ¥è¯†ç‚¹çš„ç®€è¦å›é¡¾

        ç¤ºä¾‹æ ¼å¼ï¼š
        é—®é¢˜ï¼š[æ¸…æ™°æ˜ç¡®çš„é—®é¢˜é¢˜å¹²]
        A. [é€‰é¡¹A]
        B. [é€‰é¡¹B]
        C. [é€‰é¡¹C]
        D. [é€‰é¡¹D]
        ç­”æ¡ˆï¼š[æ­£ç¡®é€‰é¡¹ï¼Œå¦‚A]
        è§£æï¼š[è¯¦ç»†è§£æï¼Œè‡³å°‘50å­—]"""},
            {"role": "user", "content": f"""æ ¹æ®ä»¥ä¸‹å­¦ä¹ å†…å®¹ï¼Œè®¾è®¡ä¸€ä¸ªé€‰æ‹©é¢˜ï¼š

        å†…å®¹ä¸»é¢˜ï¼š{slide.title}
        è¯¦ç»†å†…å®¹ï¼š{content_text}

        è¯·è®¾è®¡ï¼š
        1. ä¸€ä¸ªæ¸…æ™°æ˜ç¡®çš„é—®é¢˜é¢˜å¹²ï¼ˆåŸºäºæä¾›çš„å­¦ä¹ å†…å®¹ï¼‰
        2. 4ä¸ªé€‰é¡¹ï¼ˆAã€Bã€Cã€Dï¼‰
        3. æ­£ç¡®ç­”æ¡ˆï¼ˆæ ‡æ³¨æ¸…æ¥šï¼‰
        4. è¯¦ç»†çš„ç­”æ¡ˆè§£æï¼ˆå¿…é¡»åŒ…å«ï¼Œä¸èƒ½ä¸ºç©ºï¼‰

        è¦æ±‚ï¼š
        - é—®é¢˜å’Œé€‰é¡¹å¿…é¡»åŸºäºæä¾›çš„å­¦ä¹ å†…å®¹
        - è§£æå¿…é¡»è¯¦ç»†è¯´æ˜æ¯ä¸ªé€‰é¡¹æ­£ç¡®æˆ–é”™è¯¯çš„åŸå› 
        - æ€»å­—æ•°ä¸å°‘äº80å­—

        è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„æ ¼å¼ç”Ÿæˆã€‚"""}
        ]

        try:
            quiz, validation_result = await self.call_llm_with_validation(
                messages, task_type="quiz", expected_format="quiz_question"
            )

            if quiz and validation_result.get("passed", False):
                # è§£ææµ‹éªŒå†…å®¹å¹¶éªŒè¯ç­”æ¡ˆæ ¼å¼
                quiz_data = self._parse_and_validate_quiz(quiz, slide.title)

                if quiz_data and quiz_data["question"]:
                    quiz_data["validation"] = validation_result
                    return [quiz_data], validation_result

            return [], validation_result

        except Exception as e:
            logger.error(f"ç”Ÿæˆæµ‹éªŒå¤±è´¥: {e}")
            return [], {"passed": False, "errors": [str(e)]}

    def _clean_wikipedia_snippet(self, snippet: str) -> str:
        """æ¸…ç†Wikipediaæ‘˜è¦ç‰‡æ®µ"""
        # ç§»é™¤HTMLæ ‡ç­¾
        cleaned = re.sub(r'<[^>]+>', '', snippet)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        cleaned = re.sub(r'&\w+;', '', cleaned)
        # é™åˆ¶é•¿åº¦
        if len(cleaned) > 120:
            cleaned = cleaned[:117] + "..."
        return cleaned

    def _determine_resource_type(self, title: str) -> str:
        """æ ¹æ®æ ‡é¢˜åˆ¤æ–­èµ„æºç±»å‹"""
        title_lower = title.lower()
        if any(word in title_lower for word in ["ä¹¦", "æ•™æ", "æŒ‡å—", "æ‰‹å†Œ"]):
            return "book"
        elif any(word in title_lower for word in ["è¯¾ç¨‹", "æ•™ç¨‹", "è§†é¢‘", "ç½‘è¯¾", "mooc"]):
            return "course"
        elif any(word in title_lower for word in ["è®ºæ–‡", "ç ”ç©¶", "æœŸåˆŠ", "ä¼šè®®"]):
            return "paper"
        elif any(word in title_lower for word in ["å·¥å…·", "æ¡†æ¶", "åº“", "è½¯ä»¶"]):
            return "tool"
        else:
            return "other"

    def _parse_and_validate_quiz(self, quiz_text: str, slide_title: str) -> Optional[Dict[str, Any]]:
        """è§£æå¹¶éªŒè¯æµ‹éªŒå†…å®¹ - å¢å¼ºç‰ˆï¼Œç¡®ä¿è§£æä¸ä¸ºç©º"""
        lines = quiz_text.split("\n")
        quiz_data = {
            "question": "",
            "options": {},
            "answer": "",
            "explanation": "",
            "slide_title": slide_title
        }

        required_fields_found = {
            "question": False,
            "options_count": 0,
            "answer": False,
            "explanation": False
        }

        current_section = None
        explanation_lines = []

        for line in lines:
            line = line.strip()

            # æ£€æµ‹å„ä¸ªéƒ¨åˆ†
            if line.startswith("é—®é¢˜ï¼š") or line.startswith("é—®é¢˜:"):
                quiz_data["question"] = line[3:].strip()
                required_fields_found["question"] = True
                current_section = "question"
            elif line.startswith("A."):
                quiz_data["options"]["A"] = line[2:].strip()
                required_fields_found["options_count"] += 1
            elif line.startswith("B."):
                quiz_data["options"]["B"] = line[2:].strip()
                required_fields_found["options_count"] += 1
            elif line.startswith("C."):
                quiz_data["options"]["C"] = line[2:].strip()
                required_fields_found["options_count"] += 1
            elif line.startswith("D."):
                quiz_data["options"]["D"] = line[2:].strip()
                required_fields_found["options_count"] += 1
            elif line.startswith("ç­”æ¡ˆï¼š") or line.startswith("ç­”æ¡ˆ:"):
                quiz_data["answer"] = line[3:].strip().upper()
                required_fields_found["answer"] = True
                current_section = "answer"
            elif line.startswith("è§£æï¼š") or line.startswith("è§£æ:"):
                current_section = "explanation"
                explanation_text = line[3:].strip()
                if explanation_text:
                    explanation_lines.append(explanation_text)
                required_fields_found["explanation"] = True
            elif current_section == "explanation" and line:
                # è§£æéƒ¨åˆ†çš„åç»­è¡Œ
                explanation_lines.append(line)

        # åˆå¹¶è§£æè¡Œ
        if explanation_lines:
            quiz_data["explanation"] = " ".join(explanation_lines).strip()
            required_fields_found["explanation"] = True

        # ã€å…³é”®ä¿®å¤ã€‘å¦‚æœè§£æä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤è§£æ
        if not quiz_data["explanation"]:
            # åŸºäºé—®é¢˜å’Œç­”æ¡ˆåˆ›å»ºé»˜è®¤è§£æ
            correct_answer = quiz_data["answer"]
            correct_option = quiz_data["options"].get(correct_answer, "")

            if correct_answer and correct_option:
                # ä¸ºæ¯ä¸ªé€‰é¡¹åˆ›å»ºè§£æ
                explanations = []
                for key, option in quiz_data["options"].items():
                    if key == correct_answer:
                        explanations.append(f"{key}æ˜¯æ­£ç¡®çš„ï¼Œå› ä¸º{option}ç¬¦åˆç›¸å…³çŸ¥è¯†è¦ç‚¹ã€‚")
                    else:
                        explanations.append(f"{key}ä¸æ­£ç¡®ï¼Œå› ä¸º{option}ä¸ç›¸å…³çŸ¥è¯†ä¸ç¬¦æˆ–æœ‰è¯¯ã€‚")

                quiz_data["explanation"] = " ".join(explanations)
            else:
                # æœ€å°åŒ–é»˜è®¤è§£æ
                quiz_data["explanation"] = f"æ­£ç¡®ç­”æ¡ˆæ˜¯{correct_answer}ï¼Œå› ä¸ºè¿™æ˜¯ä¸'{slide_title}'ç›¸å…³çš„æœ€ä½³é€‰æ‹©ã€‚"

            required_fields_found["explanation"] = True

        # éªŒè¯æ˜¯å¦æ‰€æœ‰å¿…è¦å­—æ®µéƒ½å­˜åœ¨
        if (required_fields_found["question"] and
                required_fields_found["options_count"] >= 2 and  # è‡³å°‘2ä¸ªé€‰é¡¹
                required_fields_found["answer"] and
                quiz_data["answer"] in quiz_data["options"] and
                required_fields_found["explanation"] and
                quiz_data["explanation"]):
            return quiz_data

        # å¦‚æœéªŒè¯å¤±è´¥ï¼Œåˆ›å»ºå®Œæ•´çš„é»˜è®¤æµ‹éªŒ
        return self._create_default_quiz(slide_title)

    def _create_default_quiz(self, slide_title: str) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤æµ‹éªŒä½œä¸ºåå¤‡æ–¹æ¡ˆ"""
        return {
            "question": f"å…³äº{slide_title}ï¼Œä»¥ä¸‹å“ªä¸ªè¯´æ³•æ˜¯æ­£ç¡®çš„ï¼Ÿ",
            "options": {
                "A": f"{slide_title}æ˜¯æ­£ç¡®çš„å†…å®¹",
                "B": f"{slide_title}çš„éƒ¨åˆ†å†…å®¹æœ‰è¯¯",
                "C": f"æ— æ³•ç¡®å®š{slide_title}çš„å‡†ç¡®æ€§",
                "D": f"{slide_title}éœ€è¦è¿›ä¸€æ­¥éªŒè¯"
            },
            "answer": "A",
            "explanation": f"é€‰é¡¹Aæ˜¯æ­£ç¡®çš„ï¼Œå› ä¸º{slide_title}æ˜¯åŸºäºå¯é ä¿¡æ¯æä¾›çš„çŸ¥è¯†è¦ç‚¹ã€‚å…¶ä»–é€‰é¡¹æˆ–ä¸å‡†ç¡®æˆ–ä¸å®Œæ•´ã€‚",
            "slide_title": slide_title,
            "is_default": True  # æ ‡è®°ä¸ºé»˜è®¤ç”Ÿæˆçš„
        }

    def _check_python_syntax_basic(self, code: str) -> bool:
        """åŸºæœ¬Pythonè¯­æ³•æ£€æŸ¥"""
        try:
            # æ£€æŸ¥åŸºæœ¬çš„è¯­æ³•é—®é¢˜
            if "```python" in code or "```" in code:
                # æå–ä»£ç å—
                code_match = re.search(r'```(?:python)?\n(.*?)\n```', code, re.DOTALL)
                if code_match:
                    code = code_match.group(1)

            # æ£€æŸ¥å¸¸è§çš„å…³é”®å­—
            required_keywords = ["def", "class", "import", "print", "return", "if"]
            has_keyword = any(keyword in code for keyword in required_keywords[:3])

            # æ£€æŸ¥æ‹¬å·åŒ¹é…
            if code.count('(') != code.count(')'):
                return False
            if code.count('[') != code.count(']'):
                return False
            if code.count('{') != code.count('}'):
                return False

            return has_keyword

        except Exception:
            return False

    def _create_fallback_explanation(self, slide: SlideContent) -> str:
        """åˆ›å»ºé™çº§å¤„ç†çš„è§£é‡Š"""
        keywords = self._extract_keywords_from_slide(slide)[:3]
        keyword_str = "ã€".join(keywords)

        fallback_text = f"""æœ¬å¹»ç¯ç‰‡ä¸»è¦æ¶‰åŠ{keyword_str}ç­‰å†…å®¹ã€‚
ç”±äºæ¨¡å‹æ ¡éªŒæœªé€šè¿‡ï¼Œå»ºè®®å‚è€ƒåŸå§‹PPTå†…å®¹æˆ–æŸ¥é˜…ç›¸å…³èµ„æ–™è·å–æ›´å‡†ç¡®çš„ä¿¡æ¯ã€‚"""

        return fallback_text

    def _extract_keywords_from_slide(self, slide: SlideContent) -> List[str]:
        """ä»å¹»ç¯ç‰‡ä¸­æå–å…³é”®è¯ - å­¦æœ¯ä¼˜åŒ–ç‰ˆ"""
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = ""

        if slide.title:
            all_text += slide.title + " "

        if slide.content:
            for content in slide.content:
                all_text += content + " "

        if slide.bullet_points:
            for point in slide.bullet_points:
                all_text += point + " "

        if not all_text.strip():
            return []

        # 1. æå–å­¦æœ¯æœ¯è¯­ï¼ˆä¼˜å…ˆï¼‰
        academic_keywords = []
        for term in self.academic_terms:
            if term in all_text:
                academic_keywords.append(term)

        # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿå¤šçš„å­¦æœ¯æœ¯è¯­ï¼Œç›´æ¥è¿”å›
        if len(academic_keywords) >= 3:
            # å»é‡å¹¶æ’åºï¼ˆæŒ‰åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„é¡ºåºï¼‰
            seen = set()
            unique_keywords = []
            for keyword in academic_keywords:
                if keyword not in seen:
                    seen.add(keyword)
                    unique_keywords.append(keyword)
            return unique_keywords[:5]

        # 2. æå–ä¸­æ–‡å­—ç¬¦ä¸²ï¼ˆé•¿åº¦2-6çš„è¯æ±‡ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,6}', all_text)

        # 3. æå–è‹±æ–‡å•è¯ï¼ˆæŠ€æœ¯æœ¯è¯­ï¼‰
        english_words = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', all_text)  # é¦–å­—æ¯å¤§å†™çš„æœ¯è¯­
        english_words.extend(re.findall(r'\b[a-z]{3,}\b', all_text.lower()))  # å°å†™æŠ€æœ¯è¯

        # 4. æå–æ•°å­¦/æŠ€æœ¯ç¬¦å·
        tech_terms = re.findall(r'\b[A-Za-z0-9_\-\.]+\b', all_text)

        # åˆå¹¶æ‰€æœ‰å€™é€‰è¯
        all_candidates = chinese_words + english_words + tech_terms

        # è¿‡æ»¤åœç”¨è¯ï¼ˆå­¦æœ¯æ‰©å±•ç‰ˆï¼‰
        academic_stop_words = {
            # é€šç”¨åœç”¨è¯
            "çš„", "åœ¨", "å’Œ", "ä¸", "åŠ", "æˆ–", "ç­‰", "æ˜¯", "æœ‰", "åŒ…æ‹¬", "åŒ…å«",
            "è¿™ä¸ª", "é‚£ä¸ª", "è¿™äº›", "é‚£äº›", "ä¸€ç§", "ä¸€ä¸ª", "ä¸€äº›", "ä¸€", "ä¸", "ä¹Ÿ",
            "äº†", "å¾ˆ", "éƒ½", "è€Œ", "ä¸”", "ä½†", "ç„¶è€Œ", "å› æ­¤", "æ‰€ä»¥", "å› ä¸º",
            "å¦‚æœ", "é‚£ä¹ˆ", "åˆ™", "å¯ä»¥", "å¯èƒ½", "åº”è¯¥", "éœ€è¦", "è¦æ±‚",
            "å¯¹äº", "å…³äº", "æ ¹æ®", "é€šè¿‡", "ä½¿ç”¨", "åˆ©ç”¨", "é‡‡ç”¨",
            "èƒ½å¤Ÿ", "ä½¿å¾—", "ç§°ä¸º", "ç§°ä½œ", "ç§°ä¹‹ä¸º",

            # PPTç‰¹å®šåœç”¨è¯
            "ç›®å½•", "è°¢è°¢", "ä¾‹å­", "ç¤ºä¾‹", "å®ä¾‹", "å›¾è¡¨", "å›¾ç‰‡", "å›¾åƒ",
            "æ ‡é¢˜", "æ­£æ–‡", "å†…å®¹", "é¡µé¢", "å¹»ç¯ç‰‡", "é¡µæ•°", "é¡µç ",

            # é€šç”¨åŠ¨è¯
            "è¡¨ç¤º", "è¯´æ˜", "æè¿°", "è§£é‡Š", "å±•ç¤º", "æ˜¾ç¤º", "å‘ˆç°",
            "æä¾›", "ç»™å‡º", "åˆ—å‡º", "åˆ—ä¸¾", "æ€»ç»“", "æ¦‚æ‹¬",

            # æ•°å­¦é€šç”¨è¯
            "å…¬å¼", "è®¡ç®—", "æ¨å¯¼", "è¯æ˜", "æ±‚è§£", "å¾—å‡º", "å¾—åˆ°"
        }

        # è¿‡æ»¤å’Œè¯„åˆ†
        scored_keywords = {}
        for word in all_candidates:
            word_lower = word.lower()

            # è¿‡æ»¤æ¡ä»¶
            if (word not in academic_stop_words and
                    len(word) >= 2 and  # è‡³å°‘2ä¸ªå­—ç¬¦
                    not re.match(r'^\d+$', word) and  # æ’é™¤çº¯æ•°å­—
                    not re.match(r'^[a-zA-Z]$', word)):  # æ’é™¤å•ä¸ªå­—æ¯

                # è¯„åˆ†è§„åˆ™
                score = 1.0

                # å­¦æœ¯æœ¯è¯­åŠ åˆ†
                if word in self.academic_terms:
                    score += 3.0

                # æ ‡é¢˜ä¸­çš„è¯åŠ åˆ†
                if slide.title and word in slide.title:
                    score += 2.0

                # é•¿åº¦é€‚ä¸­åŠ åˆ†ï¼ˆ3-5ä¸ªå­—ç¬¦ï¼‰
                if 3 <= len(word) <= 5:
                    score += 1.0

                # è‹±æ–‡æŠ€æœ¯è¯åŠ åˆ†
                if re.match(r'^[A-Za-z]+$', word) and len(word) >= 3:
                    score += 1.0

                # é¿å…é‡å¤è¯„åˆ†
                if word not in scored_keywords or score > scored_keywords[word]:
                    scored_keywords[word] = score

        # æŒ‰åˆ†æ•°æ’åº
        sorted_keywords = sorted(scored_keywords.items(), key=lambda x: x[1], reverse=True)

        # æå–å‰5ä¸ªå…³é”®è¯
        top_keywords = [word for word, score in sorted_keywords[:5]]

        # å¦‚æœç»“æœå¤ªå°‘ï¼Œä»æ ‡é¢˜ä¸­æå–è¡¥å……
        if len(top_keywords) < 3 and slide.title:
            title_words = re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]{2,}', slide.title)
            for word in title_words:
                if (word not in top_keywords and
                        word not in academic_stop_words and
                        len(word) >= 2):
                    top_keywords.append(word)
                    if len(top_keywords) >= 5:
                        break

        return top_keywords[:5]

    async def expand_multiple_slides(self, slides: List[SlideContent]) -> List[Dict[str, Any]]:
        """æ‰©å±•å¤šä¸ªå¹»ç¯ç‰‡"""
        logger.info(f"å¼€å§‹æ‰©å±• {len(slides)} ä¸ªå¹»ç¯ç‰‡")

        tasks = [self.expand_slide(slide) for slide in slides]
        results = await asyncio.gather(*tasks)

        # ç»Ÿè®¡ä¿¡æ¯
        total_score = 0
        valid_results = 0
        extended_reading_count = 0

        for result in results:
            if "validation_score" in result:
                total_score += result["validation_score"]
                valid_results += 1

            if result.get("extended_readings") and len(result["extended_readings"]) > 0:
                extended_reading_count += 1

        if valid_results > 0:
            avg_score = total_score / valid_results
            logger.info(f"æ•´ä½“æ‰©å±•å®Œæˆï¼Œå¹³å‡æ ¡éªŒå¾—åˆ†: {avg_score:.2f}")

        logger.info(f"ğŸ“Š æ‰©å±•ç»Ÿè®¡:")
        logger.info(f"  - æˆåŠŸç”ŸæˆçŸ¥è¯†æ·±åº¦æ¢ç´¢ææ–™çš„å¹»ç¯ç‰‡: {extended_reading_count}/{len(slides)}")  # âœ… ä½¿ç”¨æ–°åç§°
        logger.info(f"  - æ•´ä½“æ ¡éªŒå¹³å‡åˆ†: {avg_score if valid_results > 0 else 0:.2f}")

        return results

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()


# å…¨å±€æ™ºèƒ½ä½“å®ä¾‹
knowledge_agent = SimpleAgent()
